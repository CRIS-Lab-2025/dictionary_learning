{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import umap\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_words(sentences):\n",
    "    words = set()\n",
    "    for sentence in sentences:\n",
    "        # Remove periods and split into words, using a regular expression\n",
    "        sentence_words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        words.update(sentence_words)\n",
    "    return words\n",
    "\n",
    "def entropy(p, base=2):\n",
    "    \"\"\"Compute Shannon entropy of distribution p (list of floats summing to 1).\"\"\"\n",
    "    log_fn = math.log if base == math.e else (lambda x: math.log(x, base))\n",
    "    H = 0.0\n",
    "    for pi in p:\n",
    "        if pi > 0:\n",
    "            H -= pi * log_fn(pi)\n",
    "    return H\n",
    "\n",
    "def mutual_information(samples, keyword1, keyword2):\n",
    "\n",
    "    num_traj = len(samples)\n",
    "\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "\n",
    "    count_joint1 = 0\n",
    "    count_joint2 = 0\n",
    "    count_joint3 = 0\n",
    "    count_joint4 = 0\n",
    "\n",
    "\n",
    "    for sen in samples:\n",
    "        if keyword1 in sen:\n",
    "            count1 += 1\n",
    "        \n",
    "        if keyword2 in sen:\n",
    "            count2 += 1\n",
    "\n",
    "        if keyword1 in sen and keyword2 in sen:\n",
    "            count_joint1 += 1\n",
    "        \n",
    "        if keyword1 not in sen and keyword2 not in sen:\n",
    "            count_joint2 += 1\n",
    "\n",
    "        if keyword1 in sen and keyword2 not in sen:\n",
    "            count_joint3 += 1\n",
    "        \n",
    "        if keyword1 not in sen and keyword2 in sen:\n",
    "            count_joint4 += 1\n",
    "\n",
    "\n",
    "    prob1 = count1 / num_traj\n",
    "    prob2 = count2 / num_traj\n",
    "    prob1and2 = count_joint1 / num_traj\n",
    "    prob1not2 = count_joint3 / num_traj\n",
    "    prob2not1 = count_joint4 / num_traj\n",
    "    probno1no2 = count_joint2 / num_traj\n",
    "\n",
    "    joint_prob = [prob1and2, prob1not2, prob2not1, probno1no2]\n",
    "    total_sum = sum(joint_prob)\n",
    "\n",
    "    # Normalize the list by dividing each element by the total sum\n",
    "    normalized_joint_prob = [prob / total_sum for prob in joint_prob]\n",
    "\n",
    "    proba = [prob1, 1-prob1]\n",
    "    total_sum = sum(proba)\n",
    "    normalized_proba = [prob / total_sum for prob in proba]\n",
    "\n",
    "    probb = [prob2, 1-prob2]\n",
    "    total_sum = sum(probb)\n",
    "    normalized_probb = [prob / total_sum for prob in probb]\n",
    "\n",
    "\n",
    "    joint_entropy = entropy(normalized_joint_prob)\n",
    "    h_a = entropy(normalized_proba)\n",
    "    h_b = entropy(normalized_probb)\n",
    "\n",
    "    mi = h_a + h_b - joint_entropy\n",
    "\n",
    "    return mi, h_a, h_b, joint_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Pythia model and tokenizer\n",
    "model_name = \"EleutherAI/pythia-410m-deduped\"  # Adjust as necessary\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate_until_period2(input_text, n, max_length=50):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Generate tokens until a period is encountered\n",
    "    generated_ids = input_ids\n",
    "    prob = []\n",
    "    while True:\n",
    "        # Get model logits for the input sequence\n",
    "        logits = model(generated_ids).logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities for the next token\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the indices of the two highest logits\n",
    "        top2_indices = torch.topk(logits, n, dim=-1).indices.squeeze(dim=0)\n",
    "        \n",
    "        # Extract the probabilities for the top 2 tokens\n",
    "        top2_probs = probs[0, top2_indices]\n",
    "        \n",
    "        # Normalize the probabilities (they should sum to 1)\n",
    "        top2_probs = top2_probs / top2_probs.sum()\n",
    "\n",
    "        # Sample a token from the top 2 probabilities\n",
    "        next_token_id = torch.multinomial(top2_probs, num_samples=1).item()\n",
    "        prob.append(top2_probs[next_token_id].item())\n",
    "        \n",
    "        # Append the generated token to the sequence\n",
    "        generated_ids = torch.cat([generated_ids, torch.tensor([[top2_indices[next_token_id]]], device=device)], dim=-1)\n",
    "        \n",
    "        # Decode the generated token\n",
    "        next_token = tokenizer.decode(top2_indices[next_token_id].item())\n",
    "        \n",
    "        # Check if the token ends with sentence-ending punctuation\n",
    "        if next_token[-1] in ['.', '!', '?']:\n",
    "            break\n",
    "        \n",
    "        # Stop if the sequence length exceeds max_length\n",
    "        if generated_ids.shape[1] > max_length:\n",
    "            break\n",
    "    \n",
    "    # Decode the full generated sequence\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"The brain is\"\n",
    "num_traj = 10000\n",
    "n=5\n",
    "\n",
    "gen_sens = []\n",
    "\n",
    "p_vals = []\n",
    "\n",
    "for i in range(num_traj):\n",
    "    generated_text, p = generate_until_period2(input_text, n)\n",
    "    print(generated_text)\n",
    "    val = np.prod(p)\n",
    "    p_vals.append(val)\n",
    "    gen_sens.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique words\n",
    "unique_words = extract_unique_words(gen_sens)\n",
    "print(len(unique_words))\n",
    "\n",
    "all_words = []\n",
    "for sentence in gen_sens:\n",
    "    # Extract words, ignore punctuation, and convert to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "    all_words.extend(words)\n",
    "\n",
    "# Step 2: Count occurrences of each word\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Step 3: Extract words that appear more than 100 times\n",
    "words_more_than_100 = [word for word, count in word_counts.items() if count > 100]\n",
    "\n",
    "# Output the words that appear more than 100 times\n",
    "print(words_more_than_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miis = []\n",
    "mii_vals = []\n",
    "unique_words = extract_unique_words(words_more_than_100)\n",
    "\n",
    "unique_words.remove('the')\n",
    "unique_words.remove('brain')\n",
    "\n",
    "for w1, w2 in itertools.combinations(unique_words, 2):\n",
    "\n",
    "    mi, h_a, h_b, h_joint = mutual_information(gen_sens, w1, w2)\n",
    "    miis.append(((w1,w2),mi))\n",
    "    mii_vals.append(mi)\n",
    "sorted_miis = sorted(miis, key=lambda x: x[1], reverse=True)  # reverse=True for descending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_vals = []\n",
    "\n",
    "for val in mii_vals:\n",
    "    if val < 1e-5:\n",
    "        pass\n",
    "    else:\n",
    "        nz_vals.append(val)\n",
    "\n",
    "\n",
    "plt.hist(nz_vals, bins=1000)\n",
    "plt.xlim(0,0.1)\n",
    "plt.xlabel('Mutual Information')\n",
    "plt.ylabel('Pairwise Counts')\n",
    "plt.ylim(0,1000)\n",
    "plt.title('70m Model: The brain is...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
