{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dictionary import AutoEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The capital of Russia is Moscow.', 'The capital of China is Beijing.', 'The capital of Greece is Athens.', 'The capital of Germany is Berlin.', 'The capital of France is Paris.', 'The capital of the United Kingdom is London.', 'The capital of Japan is Tokyo.', 'The capital of Egypt is Cairo.', 'The capital of Italy is Rome.', 'The capital of Spain is Madrid.', 'The capital of Portugal is Lisbon.', 'The capital of Canada is Ottawa.', 'The capital of Australia is Canberra.', 'The capital of Brazil is Brasília.', 'The capital of India is New Delhi.', 'The capital of the United States is Washington, D.C.', 'The capital of Argentina is Buenos Aires.', 'The capital of Mexico is Mexico City.', 'The capital of South Korea is Seoul.', 'The capital of Indonesia is Jakarta.', 'The capital of Thailand is Bangkok.', 'The capital of Norway is Oslo.', 'The capital of Sweden is Stockholm.', 'The capital of Finland is Helsinki.', 'The capital of Poland is Warsaw.', 'The capital of Austria is Vienna.']\n"
     ]
    }
   ],
   "source": [
    "# Load sentences from CSV file\n",
    "df = pd.read_csv(\"sentences.csv\", delimiter=\",\", encoding=\"utf-8\", quotechar='\"')\n",
    "sentences = df['sentence'].tolist()\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pythia model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_list = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Hook function to capture activations from the 4th MLP layer.\"\"\"\n",
    "    activation_list.append(output)\n",
    "\n",
    "# Hook 4th MLP layer (index 3)\n",
    "layer_to_hook = model.gpt_neox.layers[3].mlp\n",
    "hook = layer_to_hook.register_forward_hook(hook_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'The capital of Russia is Moscow.'\n",
      "\n",
      "Processing: 'The capital of China is Beijing.'\n",
      "\n",
      "Processing: 'The capital of Greece is Athens.'\n",
      "\n",
      "Processing: 'The capital of Germany is Berlin.'\n",
      "\n",
      "Processing: 'The capital of France is Paris.'\n",
      "\n",
      "Processing: 'The capital of the United Kingdom is London.'\n",
      "\n",
      "Processing: 'The capital of Japan is Tokyo.'\n",
      "\n",
      "Processing: 'The capital of Egypt is Cairo.'\n",
      "\n",
      "Processing: 'The capital of Italy is Rome.'\n",
      "\n",
      "Processing: 'The capital of Spain is Madrid.'\n",
      "\n",
      "Processing: 'The capital of Portugal is Lisbon.'\n",
      "\n",
      "Processing: 'The capital of Canada is Ottawa.'\n",
      "\n",
      "Processing: 'The capital of Australia is Canberra.'\n",
      "\n",
      "Processing: 'The capital of Brazil is Brasília.'\n",
      "\n",
      "Processing: 'The capital of India is New Delhi.'\n",
      "\n",
      "Processing: 'The capital of the United States is Washington, D.C.'\n",
      "\n",
      "Processing: 'The capital of Argentina is Buenos Aires.'\n",
      "\n",
      "Processing: 'The capital of Mexico is Mexico City.'\n",
      "\n",
      "Processing: 'The capital of South Korea is Seoul.'\n",
      "\n",
      "Processing: 'The capital of Indonesia is Jakarta.'\n",
      "\n",
      "Processing: 'The capital of Thailand is Bangkok.'\n",
      "\n",
      "Processing: 'The capital of Norway is Oslo.'\n",
      "\n",
      "Processing: 'The capital of Sweden is Stockholm.'\n",
      "\n",
      "Processing: 'The capital of Finland is Helsinki.'\n",
      "\n",
      "Processing: 'The capital of Poland is Warsaw.'\n",
      "\n",
      "Processing: 'The capital of Austria is Vienna.'\n",
      "Captured activations for 26 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Store per-token activations\n",
    "individual_activations = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"\\nProcessing: '{sentence}'\")\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    input_ids_batch = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Run model\n",
    "    model(**input_ids_batch)\n",
    "    \n",
    "    # Retrieve activations for this sentence\n",
    "    if activation_list:\n",
    "        activations = activation_list[-1]  # Shape: (1, seq_len, hidden_dim)\n",
    "        activations = activations.squeeze(0)  # → (seq_len, hidden_dim)\n",
    "        individual_activations.append(activations)\n",
    "    activation_list.clear()  # Clear after each sentence\n",
    "\n",
    "print(f\"Captured activations for {len(individual_activations)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahkorb/CRIS-LAB-2025/dictionary_learning/dictionary.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 26 sentences into token-aligned sparse representations.\n"
     ]
    }
   ],
   "source": [
    "# Load Dictionary Learning AutoEncoder\n",
    "ae = AutoEncoder.from_pretrained(\n",
    "    \"dictionaries/pythia-70m-deduped/mlp_out_layer3/10_32768/ae.pt\", \n",
    "    map_location=torch.device('cpu')\n",
    ")\n",
    "\n",
    "# Per-token sparse representations\n",
    "sparse_representations = []\n",
    "\n",
    "for activations in individual_activations:\n",
    "    # Encode each token separately to sparse features\n",
    "    per_token_sparse = ae.encode(activations).detach().cpu().numpy()  # (seq_len, dict_size)\n",
    "    sparse_representations.append(per_token_sparse)\n",
    "\n",
    "print(f\"Processed {len(sparse_representations)} sentences into token-aligned sparse representations.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature activations for the first sentence:\n",
      "Token: The, Top Features: [10245, 10246, 10247, 10248, 10249]\n",
      "Token: Ġcapital, Top Features: [10251, 10252, 10253, 10254, 10255]\n",
      "Token: Ġof, Top Features: [24589, 8219, 10269, 10270, 10271]\n",
      "Token: ĠRussia, Top Features: [6154, 10260, 6172, 10272, 10273]\n",
      "Token: Ġis, Top Features: [6154, 10266, 10267, 6172, 10269]\n",
      "Token: ĠMoscow, Top Features: [6154, 10276, 10277, 10278, 20519]\n",
      "Token: ., Top Features: [10245, 10246, 10247, 10248, 10249]\n",
      "\n",
      "Feature activations for the first sentence:\n",
      "Token: The, Top Features: [10245, 10246, 10247, 10248, 10249]\n",
      "Token: Ġcapital, Top Features: [10251, 10252, 10253, 10254, 10255]\n",
      "Token: Ġof, Top Features: [24589, 8219, 10269, 10270, 10271]\n",
      "Token: ĠChina, Top Features: [6154, 10260, 6172, 10272, 10273]\n",
      "Token: Ġis, Top Features: [6154, 10266, 10267, 6172, 10269]\n",
      "Token: ĠBeijing, Top Features: [6154, 10276, 10277, 10278, 20519]\n",
      "Token: ., Top Features: [10245, 10246, 10247, 10248, 10249]\n"
     ]
    }
   ],
   "source": [
    "# Analyze which features activate for specific tokens\n",
    "top_n = 800  \n",
    "top_features = []\n",
    "\n",
    "for sentence_idx, per_token_features in enumerate(sparse_representations):\n",
    "    sentence_top_features = []\n",
    "    \n",
    "    for token_idx, features in enumerate(per_token_features):\n",
    "        # Extract top N active features for this token\n",
    "        top_indices = np.argsort(features)[-top_n:][::-1]\n",
    "        sentence_top_features.append(set(top_indices))\n",
    "    \n",
    "    top_features.append(sentence_top_features)  # Store per-token top feature indices\n",
    "\n",
    "# Example: Print feature activations for each token in the first sentence\n",
    "tokenized_sentence = tokenizer(sentences[0])['input_ids']\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "print(\"\\nFeature activations for the first sentence:\")\n",
    "for token, feature_set in zip(decoded_tokens, top_features[0]):\n",
    "    print(f\"Token: {token}, Top Features: {list(feature_set)[:5]}\")  # Show top 5 features\n",
    "\n",
    "    # Example: Print feature activations for each token in the first sentence\n",
    "tokenized_sentence = tokenizer(sentences[1])['input_ids']\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "print(\"\\nFeature activations for the first sentence:\")\n",
    "for token, feature_set in zip(decoded_tokens, top_features[0]):\n",
    "    print(f\"Token: {token}, Top Features: {list(feature_set)[:5]}\")  # Show top 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output: ['The', 'Ġcapital', 'Ġof', 'ĠRussia', 'Ġis', 'ĠMoscow', '.', 'ĠMoscow', 'Ġis', 'Ġin', 'ĠRussia', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The capital of Russia is Moscow. Moscow is in Russia.\"\n",
    "tokenized_sentence = tokenizer(sentence)['input_ids']\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "print(\"Tokenized output:\", decoded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
