{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:56:53.547850Z",
     "start_time": "2025-02-14T03:56:48.815591Z"
    }
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dictionary import AutoEncoder\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sv-goat/dictionary_learning/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:56:53.578553Z",
     "start_time": "2025-02-14T03:56:53.574641Z"
    }
   },
   "source": [
    "# Load sentences from CSV file\n",
    "with open('sentences_2.txt', 'r') as f:\n",
    "    sentences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "df = pd.DataFrame(sentences, columns=['sentence'])\n",
    "sentences = df['sentence'].tolist()\n",
    "\n",
    "print(sentences)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['While eating a pizza he was annoying his sister.', 'I like pizza very much.', 'A pizza topped with mozzarella is my first choice.', \"Why don't we order pizza?\", 'We are going downtown to eat pizza.', 'Please help yourself to the pizza.', 'Pizza is my favorite food.', \"Pizza is the kind of food that fits into today's life style.\", 'Divide the pizza among you three.', \"Hey, this pizza isn't bad. Not bad at all.\", 'I could go for a nice hot pizza right now.', 'We pigged out on pizza and beer.', 'My work was to deliver pizza by motorcycle.', 'My father likes pizza very much.', 'I pigged out on pizza.', 'When I was a student, I used to go to that pizza parlor.', 'I feel like having some pizza tonight.', 'I ordered a pizza on the phone.', 'He likes such foods as tacos and pizza.', 'We pigged out on pizza and chicken at lunchtime.', \"The pasta here's pretty good. And the pizza too.\", 'My father loves pizza.', \"The pizza delivery guy hasn't come by yet.\", 'I love pizza very much.', 'After six months in China, you will realize that you regret not accepting that pizza before you left.', \"I don't understand why pepperoni pizza is so popular.\", \"Pizza is the kid's favorite meal.\", 'The children shared a pizza after school.', \"We've eaten pizza and chicken at noon.\", \"I don't like pepperoni pizza very much.\", '\"What toppings do you want on the pizza? \"\"Anything but anchovies.\"\"\"', \"I don't like pepperoni pizza that much.\", 'I like cheese pizza.', \"Hi, I'd like a deep-fried pizza roll.\", 'How do you eat a panini that was once a pizza?', 'When he asked for a single slice, they gave him an entire uncut pizza, which he proceeded to eat by rolling it up like a burrito and just shoveling it in. The question, of course, is whether a whole entity is a slice of itself.', 'I want to eat pizza tonight.', 'Divide the pizza in three.', \"At that time, the city is full of people who'll either try to sell you drugs, steal your pizza, or ask you for change.\", 'I could go for some pizza right now.', \"What's your favorite pizza topping?\", 'A pizza? Yea, that works.', 'I ate a large pizza with a friend an hour ago.', \"I don't like pizza any more than I like spaghetti.\", \"I don't like pizza anymore.\", 'Luckily they invented pizza!', 'Do you usually eat more than five slices of pizza a month?', 'Tom ate the whole pizza by himself.', 'Tom and Mary are going downtown to eat pizza.', 'When Tom opened the door, he saw Mary standing there with a six-pack and a pizza.', 'Tom put too much hot sauce on his pizza.', 'Tom even likes cold pizza.', 'Nobody can make pizza as well as Tom can.', 'Bring pizza and beer!', 'My favorite pizza is Pizza Hawaii.', 'My favorite pizza is Pizza Hawaii.', 'Her favourite food as a child was pizza.', 'I killed her by drowning her in a tub filled with Neo Kobe pizza.', 'Tomorrow, I will eat strawberry cake and pizza with my boyfriend.', 'Tom ordered a French bread pizza.', 'Please divide the pizza into three parts.', 'Please cut the pizza into three parts.', 'Please cut the pizza into three slices.', 'Pizza, please.', 'Tom is baking a pizza.', 'They bake a delicious pizza.', 'Have you ever baked a pizza?', 'Is it true that you baked a pizza today?', 'And we got a free pizza.', 'I can teach you how to bake a pizza.', 'Miss Pizza and Mr Porridge were walking in the woods.', \"It's true that Americans love pizza.\", 'Tom loves pizza and french fries.', 'Tom ate the leftover pizza for breakfast.', 'On Friday nights, I often go eat pizza with my friends.', 'I ate pizza every day last week.', 'There are two slices of pizza for each person.', 'Tom eats pizza with a fork, but Mary eats it with her hands.', 'Tom ordered pizza.', 'How large is the pizza they serve at that restaurant?', \"I'll have a kebab pizza deluxe, with mixed sauce.\", 'Does anybody want a pizza?', \"Tom doesn't want any pizza.\", 'Tom wants pepperoni on his pizza.', 'Since this is your neck of the woods, maybe you can tell us where to find a good pizza joint.', 'Making pizza is something I learned from Tom.', 'Nobody here ordered a pizza.', 'I like pizza.', 'I ordered pizza.', \"I'm ordering pizza.\", \"Where's my pizza?\", 'Can I have a pizza, please?', \"I'll go get the pizza.\", '\"How do you say \"\"pizza\"\" in Italian?\"', 'I ordered pizza by telephone.', \"You like pizza, don't you?\", 'I love pizza.', \"I don't want pizza. I'm not hungry.\", \"I had pizza for lunch, so I don't want pizza for dinner.\", \"I had pizza for lunch, so I don't want pizza for dinner.\", 'I had pizza for lunch.']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:56:54.333955Z",
     "start_time": "2025-02-14T03:56:53.673307Z"
    }
   },
   "source": [
    "# Load the Pythia model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:56:54.354133Z",
     "start_time": "2025-02-14T03:56:54.349492Z"
    }
   },
   "source": [
    "# Testing tokenizer \n",
    "sentence = \"I liike pizza\"\n",
    "tokenized_sentence = tokenizer(sentence)['input_ids']\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "print(\"Tokenized output:\", decoded_tokens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output: ['I', 'Ġli', 'ike', 'Ġpizza']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:56:56.321333Z",
     "start_time": "2025-02-14T03:56:56.317701Z"
    }
   },
   "source": [
    "activation_list = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Hook function to capture activations from the 4th MLP layer.\"\"\"\n",
    "    activation_list.append(output)\n",
    "\n",
    "# Hook 4th MLP layer (index 3)\n",
    "layer_to_hook = model.gpt_neox.layers[3].mlp\n",
    "hook = layer_to_hook.register_forward_hook(hook_fn)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:57:03.819112Z",
     "start_time": "2025-02-14T03:56:58.484686Z"
    }
   },
   "source": [
    "# Store per-token activations\n",
    "individual_activations = []\n",
    "    \n",
    "for sentence in sentences:\n",
    "    print(f\"\\nProcessing: '{sentence}'\")\n",
    "    input_ids_batch = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    model(**input_ids_batch)  # Forward pass to capture activations\n",
    "\n",
    "    if activation_list:\n",
    "        activations = activation_list[-1].squeeze(0)  # Shape: (seq_len, hidden_dim)\n",
    "        individual_activations.append(activations)\n",
    "    activation_list.clear()\n",
    "\n",
    "print(f\"Captured activations for {len(individual_activations)} sentences.\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'While eating a pizza he was annoying his sister.'\n",
      "\n",
      "Processing: 'I like pizza very much.'\n",
      "\n",
      "Processing: 'A pizza topped with mozzarella is my first choice.'\n",
      "\n",
      "Processing: 'Why don't we order pizza?'\n",
      "\n",
      "Processing: 'We are going downtown to eat pizza.'\n",
      "\n",
      "Processing: 'Please help yourself to the pizza.'\n",
      "\n",
      "Processing: 'Pizza is my favorite food.'\n",
      "\n",
      "Processing: 'Pizza is the kind of food that fits into today's life style.'\n",
      "\n",
      "Processing: 'Divide the pizza among you three.'\n",
      "\n",
      "Processing: 'Hey, this pizza isn't bad. Not bad at all.'\n",
      "\n",
      "Processing: 'I could go for a nice hot pizza right now.'\n",
      "\n",
      "Processing: 'We pigged out on pizza and beer.'\n",
      "\n",
      "Processing: 'My work was to deliver pizza by motorcycle.'\n",
      "\n",
      "Processing: 'My father likes pizza very much.'\n",
      "\n",
      "Processing: 'I pigged out on pizza.'\n",
      "\n",
      "Processing: 'When I was a student, I used to go to that pizza parlor.'\n",
      "\n",
      "Processing: 'I feel like having some pizza tonight.'\n",
      "\n",
      "Processing: 'I ordered a pizza on the phone.'\n",
      "\n",
      "Processing: 'He likes such foods as tacos and pizza.'\n",
      "\n",
      "Processing: 'We pigged out on pizza and chicken at lunchtime.'\n",
      "\n",
      "Processing: 'The pasta here's pretty good. And the pizza too.'\n",
      "\n",
      "Processing: 'My father loves pizza.'\n",
      "\n",
      "Processing: 'The pizza delivery guy hasn't come by yet.'\n",
      "\n",
      "Processing: 'I love pizza very much.'\n",
      "\n",
      "Processing: 'After six months in China, you will realize that you regret not accepting that pizza before you left.'\n",
      "\n",
      "Processing: 'I don't understand why pepperoni pizza is so popular.'\n",
      "\n",
      "Processing: 'Pizza is the kid's favorite meal.'\n",
      "\n",
      "Processing: 'The children shared a pizza after school.'\n",
      "\n",
      "Processing: 'We've eaten pizza and chicken at noon.'\n",
      "\n",
      "Processing: 'I don't like pepperoni pizza very much.'\n",
      "\n",
      "Processing: '\"What toppings do you want on the pizza? \"\"Anything but anchovies.\"\"\"'\n",
      "\n",
      "Processing: 'I don't like pepperoni pizza that much.'\n",
      "\n",
      "Processing: 'I like cheese pizza.'\n",
      "\n",
      "Processing: 'Hi, I'd like a deep-fried pizza roll.'\n",
      "\n",
      "Processing: 'How do you eat a panini that was once a pizza?'\n",
      "\n",
      "Processing: 'When he asked for a single slice, they gave him an entire uncut pizza, which he proceeded to eat by rolling it up like a burrito and just shoveling it in. The question, of course, is whether a whole entity is a slice of itself.'\n",
      "\n",
      "Processing: 'I want to eat pizza tonight.'\n",
      "\n",
      "Processing: 'Divide the pizza in three.'\n",
      "\n",
      "Processing: 'At that time, the city is full of people who'll either try to sell you drugs, steal your pizza, or ask you for change.'\n",
      "\n",
      "Processing: 'I could go for some pizza right now.'\n",
      "\n",
      "Processing: 'What's your favorite pizza topping?'\n",
      "\n",
      "Processing: 'A pizza? Yea, that works.'\n",
      "\n",
      "Processing: 'I ate a large pizza with a friend an hour ago.'\n",
      "\n",
      "Processing: 'I don't like pizza any more than I like spaghetti.'\n",
      "\n",
      "Processing: 'I don't like pizza anymore.'\n",
      "\n",
      "Processing: 'Luckily they invented pizza!'\n",
      "\n",
      "Processing: 'Do you usually eat more than five slices of pizza a month?'\n",
      "\n",
      "Processing: 'Tom ate the whole pizza by himself.'\n",
      "\n",
      "Processing: 'Tom and Mary are going downtown to eat pizza.'\n",
      "\n",
      "Processing: 'When Tom opened the door, he saw Mary standing there with a six-pack and a pizza.'\n",
      "\n",
      "Processing: 'Tom put too much hot sauce on his pizza.'\n",
      "\n",
      "Processing: 'Tom even likes cold pizza.'\n",
      "\n",
      "Processing: 'Nobody can make pizza as well as Tom can.'\n",
      "\n",
      "Processing: 'Bring pizza and beer!'\n",
      "\n",
      "Processing: 'My favorite pizza is Pizza Hawaii.'\n",
      "\n",
      "Processing: 'My favorite pizza is Pizza Hawaii.'\n",
      "\n",
      "Processing: 'Her favourite food as a child was pizza.'\n",
      "\n",
      "Processing: 'I killed her by drowning her in a tub filled with Neo Kobe pizza.'\n",
      "\n",
      "Processing: 'Tomorrow, I will eat strawberry cake and pizza with my boyfriend.'\n",
      "\n",
      "Processing: 'Tom ordered a French bread pizza.'\n",
      "\n",
      "Processing: 'Please divide the pizza into three parts.'\n",
      "\n",
      "Processing: 'Please cut the pizza into three parts.'\n",
      "\n",
      "Processing: 'Please cut the pizza into three slices.'\n",
      "\n",
      "Processing: 'Pizza, please.'\n",
      "\n",
      "Processing: 'Tom is baking a pizza.'\n",
      "\n",
      "Processing: 'They bake a delicious pizza.'\n",
      "\n",
      "Processing: 'Have you ever baked a pizza?'\n",
      "\n",
      "Processing: 'Is it true that you baked a pizza today?'\n",
      "\n",
      "Processing: 'And we got a free pizza.'\n",
      "\n",
      "Processing: 'I can teach you how to bake a pizza.'\n",
      "\n",
      "Processing: 'Miss Pizza and Mr Porridge were walking in the woods.'\n",
      "\n",
      "Processing: 'It's true that Americans love pizza.'\n",
      "\n",
      "Processing: 'Tom loves pizza and french fries.'\n",
      "\n",
      "Processing: 'Tom ate the leftover pizza for breakfast.'\n",
      "\n",
      "Processing: 'On Friday nights, I often go eat pizza with my friends.'\n",
      "\n",
      "Processing: 'I ate pizza every day last week.'\n",
      "\n",
      "Processing: 'There are two slices of pizza for each person.'\n",
      "\n",
      "Processing: 'Tom eats pizza with a fork, but Mary eats it with her hands.'\n",
      "\n",
      "Processing: 'Tom ordered pizza.'\n",
      "\n",
      "Processing: 'How large is the pizza they serve at that restaurant?'\n",
      "\n",
      "Processing: 'I'll have a kebab pizza deluxe, with mixed sauce.'\n",
      "\n",
      "Processing: 'Does anybody want a pizza?'\n",
      "\n",
      "Processing: 'Tom doesn't want any pizza.'\n",
      "\n",
      "Processing: 'Tom wants pepperoni on his pizza.'\n",
      "\n",
      "Processing: 'Since this is your neck of the woods, maybe you can tell us where to find a good pizza joint.'\n",
      "\n",
      "Processing: 'Making pizza is something I learned from Tom.'\n",
      "\n",
      "Processing: 'Nobody here ordered a pizza.'\n",
      "\n",
      "Processing: 'I like pizza.'\n",
      "\n",
      "Processing: 'I ordered pizza.'\n",
      "\n",
      "Processing: 'I'm ordering pizza.'\n",
      "\n",
      "Processing: 'Where's my pizza?'\n",
      "\n",
      "Processing: 'Can I have a pizza, please?'\n",
      "\n",
      "Processing: 'I'll go get the pizza.'\n",
      "\n",
      "Processing: '\"How do you say \"\"pizza\"\" in Italian?\"'\n",
      "\n",
      "Processing: 'I ordered pizza by telephone.'\n",
      "\n",
      "Processing: 'You like pizza, don't you?'\n",
      "\n",
      "Processing: 'I love pizza.'\n",
      "\n",
      "Processing: 'I don't want pizza. I'm not hungry.'\n",
      "\n",
      "Processing: 'I had pizza for lunch, so I don't want pizza for dinner.'\n",
      "\n",
      "Processing: 'I had pizza for lunch, so I don't want pizza for dinner.'\n",
      "\n",
      "Processing: 'I had pizza for lunch.'\n",
      "Captured activations for 101 sentences.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:57:04.726494Z",
     "start_time": "2025-02-14T03:57:03.851465Z"
    }
   },
   "source": [
    "# Load Dictionary Learning AutoEncoder\n",
    "ae = AutoEncoder.from_pretrained(\n",
    "    \"dictionaries/pythia-70m-deduped/mlp_out_layer3/10_32768/ae.pt\",\n",
    "    # Let torch automatially choose device based on asvail.\n",
    "    # map_location=torch.device('cpu')\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:57:12.835793Z",
     "start_time": "2025-02-14T03:57:09.246335Z"
    }
   },
   "source": [
    "# Convert activations to sparse representations\n",
    "sparse_representations = []\n",
    "for activations in individual_activations:\n",
    "    sparse_repr = ae.encode(activations).detach().cpu().numpy()  # (seq_len, dict_size)\n",
    "    sparse_representations.append(sparse_repr)\n",
    "print(f\"Processed {len(sparse_representations)} sentences into token-aligned sparse representations.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 101 sentences into token-aligned sparse representations.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:57:20.641755Z",
     "start_time": "2025-02-14T03:57:17.243096Z"
    }
   },
   "source": [
    "# Aggregate features: Find top activated features across all tokens in all sentences\n",
    "avg_feature_weights = {}\n",
    "ovr_features = set([])\n",
    "for sentence_features in sparse_representations:\n",
    "    curr_sentence_features = set([])  # Reset curr_sentence_features for each sentence[]\n",
    "    i = 0\n",
    "    for token_features in sentence_features:\n",
    "        # Can features be negative???\n",
    "        # Does this 800 make sense anymore?\n",
    "        top_indices = np.argsort(token_features)[-100:][::-1]  # Top 800 features per token\n",
    "        # Aggregate all the features across a sentence. Do set intersection afterwards so that only feature common across all the sentences are considered. We'll also maintain the average value for all the seen features. then, we'll get the decoded output with these valeus. Then, we'll scale those weights alone by 10x. using  this new model, we'll do text generation and see if the feature common across all sentences shows up a lot, which would confirm our hypothesis. \n",
    "        # for idx in top_indices:\n",
    "        #     feature_counts[idx] = feature_counts.get(idx, 0) + 1\n",
    "        # Aggregate features across sentences. \n",
    "        for idx in top_indices:\n",
    "            curr_sentence_features.add(idx)\n",
    "            # This might get too big. \n",
    "            avg_feature_weights[idx] = [0, 0] if idx not in avg_feature_weights else [avg_feature_weights[idx][0] + token_features[idx], avg_feature_weights[idx][1] + 1]\n",
    "            \n",
    "    if len(ovr_features) == 0:\n",
    "        ovr_features = set(curr_sentence_features)\n",
    "    else:\n",
    "        # Adding  this print statement so that we don't get here everytime, which means that there is nothing common across all the sentences\n",
    "        i += 1\n",
    "        if i > 1:\n",
    "            print(\"Entering else\")\n",
    "        # Maybe try 80% threshold? \n",
    "        # i guess the best way would be to read the anthropic paper. \n",
    "        ovr_features = ovr_features.intersection(set(curr_sentence_features))\n",
    "    \n",
    "    \n",
    "            "
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:57:21.441222Z",
     "start_time": "2025-02-14T03:57:21.432933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the average weights of the features. \n",
    "avg_feature_weights = {k: v[0] / v[1] for k, v in avg_feature_weights.items() if k in ovr_features}\n",
    "\n",
    "# Print subset for sanity\n",
    "print(len(avg_feature_weights))\n",
    "print(len(ovr_features))\n",
    "\n",
    "# Print 5 items of avg feature weights\n",
    "print(list(avg_feature_weights.items())[:5])\n",
    "\n",
    "# Print 5 indices from itnersection\n",
    "print(list(ovr_features)[:5])\n",
    "\n",
    "# Only 6 common vectors? COuld be a problem somewhere. "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "[(np.int64(20383), np.float32(14.459406)), (np.int64(21462), np.float32(1.9070497)), (np.int64(7082), np.float32(2.792452)), (np.int64(18192), np.float32(4.6524243)), (np.int64(30484), np.float32(0.6164037))]\n",
      "[np.int64(7082), np.int64(18192), np.int64(30484), np.int64(21462), np.int64(9690)]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:57:30.085659Z",
     "start_time": "2025-02-14T03:57:30.063611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Decoding\n",
    "# COnstruct a sparse vector with the common features and average weights\n",
    "sparse_vector = np.zeros((32768,))  # Assume dictionary size is 32768\n",
    "for idx in ovr_features:\n",
    "    sparse_vector[idx] = avg_feature_weights[idx]\n",
    "    \n",
    "\n",
    "# Decode the sparse vector\n",
    "decoded_activations = ae.decode(torch.tensor(sparse_vector).float())"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T22:30:52.336108Z",
     "start_time": "2025-02-13T22:30:52.332093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# What are the weights present in the mlp layer?\n",
    "print(dir(model.gpt_neox.layers[3]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_is_hf_initialized', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'attention', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'input_layernorm', 'ipu', 'load_state_dict', 'mlp', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'post_attention_dropout', 'post_attention_layernorm', 'post_mlp_dropout', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'use_parallel_residual', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:57:34.424948Z",
     "start_time": "2025-02-14T03:57:34.417545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_dict = model.gpt_neox.layers[3].state_dict()\n",
    "print(state_dict.keys())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['input_layernorm.weight', 'input_layernorm.bias', 'post_attention_layernorm.weight', 'post_attention_layernorm.bias', 'attention.query_key_value.weight', 'attention.query_key_value.bias', 'attention.dense.weight', 'attention.dense.bias', 'mlp.dense_h_to_4h.weight', 'mlp.dense_h_to_4h.bias', 'mlp.dense_4h_to_h.weight', 'mlp.dense_4h_to_h.bias'])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T22:36:53.929689Z",
     "start_time": "2025-02-13T22:36:53.897165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the og weights\n",
    "torch.save(state_dict, \"og_weights.pt\")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:59:32.874467Z",
     "start_time": "2025-02-14T03:59:32.837385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the og weights\n",
    "state_dict = torch.load(\"og_weights.pt\")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:59:34.562931Z",
     "start_time": "2025-02-14T03:59:34.560415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight = state_dict['mlp.dense_4h_to_h.weight']\n",
    "bias = state_dict['mlp.dense_4h_to_h.bias']\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T22:44:13.410990Z",
     "start_time": "2025-02-13T22:44:13.078314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Outputs before my stuff\n",
    "test_input = \"I like\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\", padding=\"max_length\", truncation = True)\n",
    "attention_mask = inputs['attention_mask']  # Get the attention_mask\n",
    "with torch.no_grad():\n",
    "    output = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "# Print decoded output\n",
    "print(decoded_output)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like the way I'm doing it. I'm not sure what to do with it. I'm not\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:58:23.083852Z",
     "start_time": "2025-02-14T03:58:23.079388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acts_np = decoded_activations.detach().numpy()\n",
    "# Arbitrary threshold\n",
    "print(len(acts_np[acts_np > 1]))\n",
    "# non_zero_indices = np.nonzero(decoded_activations)\n",
    "# print(non_zero_indices.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:58:33.940439Z",
     "start_time": "2025-02-14T03:58:33.936465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acts_np_thresh = acts_np[acts_np > 1]\n",
    "print(acts_np_thresh.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35,)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T03:59:47.964863Z",
     "start_time": "2025-02-14T03:59:47.952247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Weight scaling\n",
    "\n",
    "\n",
    "# What part of the decoded outputs are significant?\n",
    "# Considering non-zero indices for now\n",
    "\n",
    "# Scale all the indices by 10\n",
    "# weight[acts_np_thresh] *= 5\n",
    "\n",
    "# Add 5 bias to all the non-zero indices\n",
    "bias[acts_np_thresh] += 5\n",
    "\n",
    "# write back to the model\n",
    "state_dict['mlp.dense_4h_to_h.weight'] = weight\n",
    "state_dict['mlp.dense_4h_to_h.bias'] = bias\n",
    "\n",
    "model.gpt_neox.layers[3].load_state_dict(state_dict)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T04:00:00.132915Z",
     "start_time": "2025-02-14T03:59:59.010241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  Test out hypothesis\n",
    "\n",
    "# Tokenize the input\n",
    "\n",
    "# Is there a better way to do this?\n",
    "# Pass the tokenized input through the model\n",
    "# Tokenize the input\n",
    "test_input = \"I like\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\", padding=\"max_length\", truncation = True)\n",
    "attention_mask = inputs['attention_mask']  # Get the attention_mask\n",
    "with torch.no_grad():\n",
    "    output = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "# Print decoded output\n",
    "print(decoded_output)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to use a bit but I'm not sure to to use it. However, I'm sure to\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 92,
   "source": [
    "# # Analyze which features activate for specific tokens\n",
    "# top_n = 800  \n",
    "# top_features = []\n",
    "\n",
    "# for sentence_idx, per_token_features in enumerate(sparse_representations):\n",
    "#     sentence_top_features = []\n",
    "    \n",
    "#     for token_idx, features in enumerate(per_token_features):\n",
    "#         # Extract top N active features for this token\n",
    "#         top_indices = np.argsort(features)[-top_n:][::-1]\n",
    "#         sentence_top_features.append(set(top_indices))\n",
    "    \n",
    "#     top_features.append(sentence_top_features)  # Store per-token top feature indices\n",
    "\n",
    "# # Example: Print feature activations for each token in the first sentence\n",
    "# tokenized_sentence = tokenizer(sentences[0])['input_ids']\n",
    "# decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "# print(\"\\nFeature activations for the first sentence:\")\n",
    "# for token, feature_set in zip(decoded_tokens, top_features[0]):\n",
    "#     print(f\"Token: {token}, Top Features: {list(feature_set)[:10]}\")  # Show top 5 features\n",
    "\n",
    "#     # Example: Print feature activations for each token in the first sentence\n",
    "# tokenized_sentence = tokenizer(sentences[1])['input_ids']\n",
    "# decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "# print(\"\\nFeature activations for the first sentence:\")\n",
    "# for token, feature_set in zip(decoded_tokens, top_features[0]):\n",
    "#     print(f\"Token: {token}, Top Features: {list(feature_set)[:10]}\")  # Show top 5 features\n",
    "    \n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:37:36.224345Z",
     "start_time": "2025-02-13T20:37:36.220370Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 18,
   "source": [
    "# Select the most frequently occurring features\n",
    "top_common_features = sorted(feature_counts, key=feature_counts.get, reverse=True)[:800]\n",
    "\n",
    "# # Create a synthetic sparse vector using these common features\n",
    "# synthetic_sparse_vector = np.zeros((32768,))  # Assume dictionary size is 32768\n",
    "# for idx in top_common_features:\n",
    "#     synthetic_sparse_vector[idx] = 1  # Set these features as active"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T20:37:46.213174Z",
     "start_time": "2025-02-13T20:37:46.210544Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(10900), np.int64(10969), np.int64(10968), np.int64(10918), np.int64(10919), np.int64(10967), np.int64(10966), np.int64(10920), np.int64(10965), np.int64(10921)]\n"
     ]
    }
   ],
   "execution_count": 19,
   "source": "print(top_common_features[:10])"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode sparse vector back into model space\n",
    "synthetic_dense_vector = ae.decode(torch.tensor(synthetic_sparse_vector).float()).detach().cpu()\n",
    "synthetic_dense_vector *= 10  # Experiment with scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input: ['The', 'Ġcapital', 'Ġof', 'Ġ', '<XYZ>', 'Ġis']\n"
     ]
    }
   ],
   "source": [
    "# Add a new special token\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [\"<XYZ>\"]})\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to include new token\n",
    "\n",
    "masked_sentence = \"The capital of <XYZ> is\"\n",
    "input_ids = tokenizer(masked_sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Convert token IDs to tokens\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(f\"Tokenized input: {decoded_tokens}\")  # Debugging\n",
    "\n",
    "# **Find the placeholder index**\n",
    "try:\n",
    "    placeholder_index = decoded_tokens.index(\"<XYZ>\")\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Could not find placeholder token in: {decoded_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token:  the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert input_ids to embeddings\n",
    "model_inputs = model.get_input_embeddings()(input_ids)\n",
    "\n",
    "# Inject synthetic feature vector at the placeholder position\n",
    "model_inputs[:, placeholder_index, :] = synthetic_dense_vector\n",
    "\n",
    "# Generate text from modified embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs_embeds=model_inputs)\n",
    "    logits = outputs.logits[:, -1, :]  # Get last token logits\n",
    "    predicted_token_id = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "# Decode the predicted token\n",
    "predicted_word = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"Predicted token: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
