{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dictionary import AutoEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Did you know llamas are domesticated animals native to South America?', 'Llamas are actually part of the camelid family, along with alpacas, guanacos, and vicuñas.', 'People have used llamas as pack animals in the Andes for centuries.', 'A llama can carry about 25-30% of its body weight, which is pretty impressive.', 'Llamas are usually calm, but if you annoy them, they’ll spit at you—fair warning!', 'Their wool is great for making textiles, though llama wool is a bit coarser than alpaca wool.', 'It’s funny how llamas hum to communicate with each other, right?', 'Llamas have this cool three-chambered stomach that helps them digest tough plants.', 'They’re super social too—llamas love being in herds.', 'An adult llama typically weighs somewhere between 250 and 450 pounds.', 'Llamas can live up to 20 or even 25 years, which is longer than I expected!', 'Their eyesight is amazing—llamas can spot predators from really far away.', 'A lot of farmers use llamas to guard their sheep and other livestock, which I didn’t know until recently.', 'Llamas are actually pretty smart and easy to train if you’re patient and gentle with them.', 'With their padded feet, llamas can walk on rough terrain without messing up the ground.', 'Llamas have a unique immune system, and scientists study their tiny, tough antibodies all the time.', 'Oh, and baby llamas? They’re called crias—adorable, right?', 'If they need to, llamas can run up to 35 miles an hour, which is wild for such a chill animal.', 'Llamas have this whole social hierarchy within their herds—it’s fascinating.', 'They’ve even been showing up in Andean art for thousands of years—llamas have been iconic forever.']\n"
     ]
    }
   ],
   "source": [
    "# Load sentences from CSV file\n",
    "df = pd.read_csv(\"sentences.csv\", delimiter=\",\", encoding=\"utf-8\", quotechar='\"')\n",
    "sentences = df['sentence'].tolist()\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Pythia model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_tokens(['llama','llamas','Llamas','Llama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output: ['With', 'Ġtheir', 'Ġp', 'added', 'Ġfeet', ',', 'Ġ', 'llamas', 'Ġcan', 'Ġwalk', 'Ġon', 'Ġrough', 'Ġterrain', 'Ġwithout', 'Ġmess', 'ing', 'Ġup', 'Ġthe', 'Ġground', '.']\n"
     ]
    }
   ],
   "source": [
    "# Testing tokenizer \n",
    "sentence = \"With their padded feet, llamas can walk on rough terrain without messing up the ground.\"\n",
    "tokenized_sentence = tokenizer(sentence)['input_ids']\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "print(\"Tokenized output:\", decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_list = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Hook function to capture activations from the 4th MLP layer.\"\"\"\n",
    "    activation_list.append(output)\n",
    "\n",
    "# Hook 4th MLP layer (index 3)\n",
    "layer_to_hook = model.gpt_neox.layers[3].mlp\n",
    "hook = layer_to_hook.register_forward_hook(hook_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'The capital of Russia is Moscow.'\n",
      "\n",
      "Processing: 'The capital of China is Beijing.'\n",
      "\n",
      "Processing: 'The capital of Greece is Athens.'\n",
      "\n",
      "Processing: 'The capital of Germany is Berlin.'\n",
      "\n",
      "Processing: 'The capital of France is Paris.'\n",
      "\n",
      "Processing: 'The capital of the United Kingdom is London.'\n",
      "\n",
      "Processing: 'The capital of Japan is Tokyo.'\n",
      "\n",
      "Processing: 'The capital of Egypt is Cairo.'\n",
      "\n",
      "Processing: 'The capital of Italy is Rome.'\n",
      "\n",
      "Processing: 'The capital of Spain is Madrid.'\n",
      "\n",
      "Processing: 'The capital of Portugal is Lisbon.'\n",
      "\n",
      "Processing: 'The capital of Canada is Ottawa.'\n",
      "\n",
      "Processing: 'The capital of Australia is Canberra.'\n",
      "\n",
      "Processing: 'The capital of Brazil is Brasília.'\n",
      "\n",
      "Processing: 'The capital of India is New Delhi.'\n",
      "\n",
      "Processing: 'The capital of the United States is Washington, D.C.'\n",
      "\n",
      "Processing: 'The capital of Argentina is Buenos Aires.'\n",
      "\n",
      "Processing: 'The capital of Mexico is Mexico City.'\n",
      "\n",
      "Processing: 'The capital of South Korea is Seoul.'\n",
      "\n",
      "Processing: 'The capital of Indonesia is Jakarta.'\n",
      "\n",
      "Processing: 'The capital of Thailand is Bangkok.'\n",
      "\n",
      "Processing: 'The capital of Norway is Oslo.'\n",
      "\n",
      "Processing: 'The capital of Sweden is Stockholm.'\n",
      "\n",
      "Processing: 'The capital of Finland is Helsinki.'\n",
      "\n",
      "Processing: 'The capital of Poland is Warsaw.'\n",
      "\n",
      "Processing: 'The capital of Austria is Vienna.'\n",
      "Captured activations for 26 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Store per-token activations\n",
    "individual_activations = []\n",
    "    \n",
    "for sentence in sentences:\n",
    "    print(f\"\\nProcessing: '{sentence}'\")\n",
    "    input_ids_batch = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    model(**input_ids_batch)  # Forward pass to capture activations\n",
    "\n",
    "    if activation_list:\n",
    "        activations = activation_list[-1].squeeze(0)  # Shape: (seq_len, hidden_dim)\n",
    "        individual_activations.append(activations)\n",
    "    activation_list.clear()\n",
    "\n",
    "print(f\"Captured activations for {len(individual_activations)} sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryansherby/Library/CloudStorage/OneDrive-Personal/Documents/Columbia/CRIS Lab Project/dictionary_learning/dictionary.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = t.load(path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "# Load Dictionary Learning AutoEncoder\n",
    "ae = AutoEncoder.from_pretrained(\n",
    "    \"dictionaries/pythia-70m-deduped/mlp_out_layer3/10_32768/ae.pt\", \n",
    "    map_location=torch.device('cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 26 sentences into token-aligned sparse representations.\n"
     ]
    }
   ],
   "source": [
    "# Convert activations to sparse representations\n",
    "sparse_representations = []\n",
    "for activations in individual_activations:\n",
    "    sparse_repr = ae.encode(activations).detach().cpu().numpy()  # (seq_len, dict_size)\n",
    "    sparse_representations.append(sparse_repr)\n",
    "print(f\"Processed {len(sparse_representations)} sentences into token-aligned sparse representations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate features: Find top activated features across all tokens in all sentences\n",
    "feature_counts = {}\n",
    "for sentence_features in sparse_representations:\n",
    "    for token_features in sentence_features:\n",
    "        top_indices = np.argsort(token_features)[-800:][::-1]  # Top 800 features per token\n",
    "        for idx in top_indices:\n",
    "            feature_counts[idx] = feature_counts.get(idx, 0) + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20383: 52,\n",
       " 18192: 52,\n",
       " 21462: 48,\n",
       " 7082: 27,\n",
       " 29597: 28,\n",
       " 26654: 52,\n",
       " 9312: 51,\n",
       " 3959: 30,\n",
       " 10395: 177,\n",
       " 19599: 37,\n",
       " 10923: 199,\n",
       " 10925: 199,\n",
       " 10924: 199,\n",
       " 10920: 199,\n",
       " 10922: 199,\n",
       " 10921: 199,\n",
       " 10919: 199,\n",
       " 10918: 199,\n",
       " 10917: 199,\n",
       " 10926: 199,\n",
       " 32767: 177,\n",
       " 10927: 199,\n",
       " 10916: 199,\n",
       " 10929: 199,\n",
       " 10930: 199,\n",
       " 10931: 199,\n",
       " 10932: 199,\n",
       " 10933: 199,\n",
       " 10934: 199,\n",
       " 10935: 199,\n",
       " 10936: 199,\n",
       " 10937: 199,\n",
       " 10938: 199,\n",
       " 10939: 199,\n",
       " 10940: 199,\n",
       " 10941: 199,\n",
       " 10928: 199,\n",
       " 10914: 199,\n",
       " 10915: 199,\n",
       " 10943: 199,\n",
       " 10887: 199,\n",
       " 10888: 199,\n",
       " 10889: 199,\n",
       " 10890: 199,\n",
       " 10891: 199,\n",
       " 10892: 199,\n",
       " 10893: 199,\n",
       " 10894: 199,\n",
       " 10895: 199,\n",
       " 10896: 199,\n",
       " 10897: 199,\n",
       " 10898: 199,\n",
       " 10899: 199,\n",
       " 10900: 199,\n",
       " 10901: 199,\n",
       " 10902: 199,\n",
       " 10903: 199,\n",
       " 10904: 199,\n",
       " 10905: 199,\n",
       " 10906: 199,\n",
       " 10907: 199,\n",
       " 10908: 199,\n",
       " 10909: 199,\n",
       " 10910: 199,\n",
       " 10911: 199,\n",
       " 10912: 199,\n",
       " 10913: 199,\n",
       " 10942: 199,\n",
       " 10945: 199,\n",
       " 10944: 199,\n",
       " 10977: 199,\n",
       " 10979: 199,\n",
       " 10980: 199,\n",
       " 10981: 199,\n",
       " 10982: 199,\n",
       " 10983: 199,\n",
       " 10984: 199,\n",
       " 10985: 199,\n",
       " 10986: 199,\n",
       " 10987: 199,\n",
       " 10988: 199,\n",
       " 10989: 199,\n",
       " 10990: 199,\n",
       " 10991: 199,\n",
       " 10992: 199,\n",
       " 10993: 199,\n",
       " 10994: 199,\n",
       " 10995: 199,\n",
       " 10996: 199,\n",
       " 10997: 199,\n",
       " 10998: 199,\n",
       " 10999: 199,\n",
       " 11000: 199,\n",
       " 11001: 199,\n",
       " 11002: 199,\n",
       " 11003: 199,\n",
       " 11004: 199,\n",
       " 11005: 199,\n",
       " 11006: 199,\n",
       " 11007: 199,\n",
       " 10978: 199,\n",
       " 10976: 199,\n",
       " 10885: 199,\n",
       " 10975: 199,\n",
       " 10946: 199,\n",
       " 10947: 199,\n",
       " 10948: 199,\n",
       " 10949: 199,\n",
       " 10950: 199,\n",
       " 10951: 199,\n",
       " 10952: 199,\n",
       " 10953: 199,\n",
       " 10954: 199,\n",
       " 10955: 199,\n",
       " 10956: 199,\n",
       " 10957: 199,\n",
       " 10958: 199,\n",
       " 10959: 199,\n",
       " 10960: 199,\n",
       " 10961: 199,\n",
       " 10962: 199,\n",
       " 10963: 199,\n",
       " 10964: 199,\n",
       " 10965: 199,\n",
       " 10966: 199,\n",
       " 10967: 199,\n",
       " 10968: 199,\n",
       " 10969: 199,\n",
       " 10970: 199,\n",
       " 10971: 199,\n",
       " 10972: 199,\n",
       " 10973: 199,\n",
       " 10974: 199,\n",
       " 10886: 199,\n",
       " 10881: 199,\n",
       " 10884: 199,\n",
       " 10883: 199,\n",
       " 10790: 191,\n",
       " 10791: 191,\n",
       " 10792: 191,\n",
       " 10793: 192,\n",
       " 10794: 193,\n",
       " 10795: 192,\n",
       " 10796: 192,\n",
       " 10797: 192,\n",
       " 10798: 192,\n",
       " 10799: 194,\n",
       " 10800: 194,\n",
       " 10801: 194,\n",
       " 10802: 194,\n",
       " 10803: 194,\n",
       " 10804: 194,\n",
       " 10805: 194,\n",
       " 10806: 194,\n",
       " 10807: 195,\n",
       " 10808: 197,\n",
       " 10809: 195,\n",
       " 10810: 195,\n",
       " 10811: 195,\n",
       " 10812: 197,\n",
       " 10813: 198,\n",
       " 10814: 198,\n",
       " 10815: 198,\n",
       " 10816: 198,\n",
       " 10817: 198,\n",
       " 10818: 199,\n",
       " 10789: 191,\n",
       " 10788: 190,\n",
       " 10787: 189,\n",
       " 10771: 111,\n",
       " 10758: 53,\n",
       " 10759: 53,\n",
       " 10760: 56,\n",
       " 10761: 83,\n",
       " 10762: 83,\n",
       " 10763: 85,\n",
       " 10764: 85,\n",
       " 10765: 85,\n",
       " 10766: 89,\n",
       " 10767: 92,\n",
       " 10768: 95,\n",
       " 10769: 99,\n",
       " 10770: 105,\n",
       " 10772: 116,\n",
       " 10786: 188,\n",
       " 10773: 119,\n",
       " 10774: 128,\n",
       " 10775: 132,\n",
       " 10776: 133,\n",
       " 10777: 162,\n",
       " 10778: 171,\n",
       " 10779: 176,\n",
       " 10780: 176,\n",
       " 10781: 179,\n",
       " 10782: 181,\n",
       " 10783: 185,\n",
       " 10784: 186,\n",
       " 10785: 186,\n",
       " 10819: 199,\n",
       " 10820: 199,\n",
       " 10821: 199,\n",
       " 10868: 199,\n",
       " 10855: 199,\n",
       " 10856: 199,\n",
       " 10857: 199,\n",
       " 10858: 199,\n",
       " 10859: 199,\n",
       " 10860: 199,\n",
       " 10861: 199,\n",
       " 10862: 199,\n",
       " 10863: 199,\n",
       " 10864: 199,\n",
       " 10865: 199,\n",
       " 10866: 199,\n",
       " 10867: 199,\n",
       " 10869: 199,\n",
       " 10853: 199,\n",
       " 10870: 199,\n",
       " 10871: 199,\n",
       " 10872: 199,\n",
       " 10873: 199,\n",
       " 10874: 199,\n",
       " 10875: 199,\n",
       " 10876: 199,\n",
       " 10877: 199,\n",
       " 10878: 199,\n",
       " 10879: 199,\n",
       " 10880: 199,\n",
       " 11009: 199,\n",
       " 10882: 199,\n",
       " 10854: 199,\n",
       " 10852: 199,\n",
       " 10822: 199,\n",
       " 10836: 199,\n",
       " 10823: 199,\n",
       " 10824: 199,\n",
       " 10825: 199,\n",
       " 10826: 199,\n",
       " 10827: 199,\n",
       " 10828: 199,\n",
       " 10829: 199,\n",
       " 10830: 199,\n",
       " 10831: 199,\n",
       " 10832: 199,\n",
       " 10833: 199,\n",
       " 10834: 199,\n",
       " 10835: 199,\n",
       " 10837: 199,\n",
       " 10851: 199,\n",
       " 10838: 199,\n",
       " 10839: 199,\n",
       " 10840: 199,\n",
       " 10841: 199,\n",
       " 10842: 199,\n",
       " 10843: 199,\n",
       " 10844: 199,\n",
       " 10845: 199,\n",
       " 10846: 199,\n",
       " 10847: 199,\n",
       " 10848: 199,\n",
       " 10849: 199,\n",
       " 10850: 199,\n",
       " 11008: 199,\n",
       " 11014: 199,\n",
       " 11010: 199,\n",
       " 11183: 197,\n",
       " 11170: 199,\n",
       " 11171: 199,\n",
       " 11172: 199,\n",
       " 11173: 199,\n",
       " 11174: 199,\n",
       " 11175: 199,\n",
       " 11176: 199,\n",
       " 11177: 199,\n",
       " 11178: 199,\n",
       " 11179: 199,\n",
       " 11180: 198,\n",
       " 11181: 198,\n",
       " 11182: 197,\n",
       " 11184: 196,\n",
       " 11168: 199,\n",
       " 11185: 195,\n",
       " 11186: 192,\n",
       " 11187: 192,\n",
       " 11188: 192,\n",
       " 11189: 192,\n",
       " 11190: 190,\n",
       " 11191: 188,\n",
       " 11192: 184,\n",
       " 11193: 182,\n",
       " 11194: 178,\n",
       " 11195: 172,\n",
       " 11196: 171,\n",
       " 11197: 168,\n",
       " 11169: 199,\n",
       " 11167: 199,\n",
       " 11011: 199,\n",
       " 11151: 199,\n",
       " 11138: 199,\n",
       " 11139: 199,\n",
       " 11140: 199,\n",
       " 11141: 199,\n",
       " 11142: 199,\n",
       " 11143: 199,\n",
       " 11144: 199,\n",
       " 11145: 199,\n",
       " 11146: 199,\n",
       " 11147: 199,\n",
       " 11148: 199,\n",
       " 11149: 199,\n",
       " 11150: 199,\n",
       " 11152: 199,\n",
       " 11166: 199,\n",
       " 11153: 199,\n",
       " 11154: 199,\n",
       " 11155: 199,\n",
       " 11156: 199,\n",
       " 11157: 199,\n",
       " 11158: 199,\n",
       " 11159: 199,\n",
       " 11160: 199,\n",
       " 11161: 199,\n",
       " 11162: 199,\n",
       " 11163: 199,\n",
       " 11164: 199,\n",
       " 11165: 199,\n",
       " 11198: 141,\n",
       " 11199: 138,\n",
       " 11200: 135,\n",
       " 11246: 52,\n",
       " 11233: 53,\n",
       " 11234: 53,\n",
       " 11235: 53,\n",
       " 11236: 53,\n",
       " 11237: 53,\n",
       " 11238: 53,\n",
       " 11239: 54,\n",
       " 11240: 54,\n",
       " 11241: 53,\n",
       " 11242: 53,\n",
       " 11243: 52,\n",
       " 11244: 52,\n",
       " 11245: 52,\n",
       " 11247: 50,\n",
       " 11201: 132,\n",
       " 11248: 50,\n",
       " 11249: 48,\n",
       " 11250: 44,\n",
       " 11251: 40,\n",
       " 11252: 32,\n",
       " 11253: 30,\n",
       " 11254: 27,\n",
       " 11255: 26,\n",
       " 11256: 26,\n",
       " 11257: 26,\n",
       " 11258: 26,\n",
       " 11259: 26,\n",
       " 11260: 26,\n",
       " 11232: 53,\n",
       " 11231: 53,\n",
       " 11230: 53,\n",
       " 11229: 54,\n",
       " 11202: 130,\n",
       " 11203: 128,\n",
       " 11204: 119,\n",
       " 11205: 117,\n",
       " 11206: 113,\n",
       " 11207: 109,\n",
       " 11208: 108,\n",
       " 11209: 106,\n",
       " 11210: 104,\n",
       " 11211: 102,\n",
       " 11212: 97,\n",
       " 11213: 97,\n",
       " 11214: 91,\n",
       " 11215: 91,\n",
       " 11216: 90,\n",
       " 11217: 88,\n",
       " 11218: 87,\n",
       " 11219: 86,\n",
       " 11220: 86,\n",
       " 11221: 83,\n",
       " 11222: 83,\n",
       " 11223: 82,\n",
       " 11224: 81,\n",
       " 11225: 57,\n",
       " 11226: 54,\n",
       " 11227: 54,\n",
       " 11228: 54,\n",
       " 11137: 199,\n",
       " 11136: 199,\n",
       " 11135: 199,\n",
       " 11056: 199,\n",
       " 11043: 199,\n",
       " 11044: 199,\n",
       " 11045: 199,\n",
       " 11046: 199,\n",
       " 11047: 199,\n",
       " 11048: 199,\n",
       " 11049: 199,\n",
       " 11050: 199,\n",
       " 11051: 199,\n",
       " 11052: 199,\n",
       " 11053: 199,\n",
       " 11054: 199,\n",
       " 11055: 199,\n",
       " 11057: 199,\n",
       " 11072: 199,\n",
       " 11058: 199,\n",
       " 11059: 199,\n",
       " 11060: 199,\n",
       " 11061: 199,\n",
       " 11062: 199,\n",
       " 11063: 199,\n",
       " 11064: 199,\n",
       " 11065: 199,\n",
       " 11066: 199,\n",
       " 11067: 199,\n",
       " 11068: 199,\n",
       " 11069: 199,\n",
       " 11070: 199,\n",
       " 11042: 199,\n",
       " 11041: 199,\n",
       " 11040: 199,\n",
       " 11039: 199,\n",
       " 11012: 199,\n",
       " 11013: 199,\n",
       " 10756: 53,\n",
       " 11015: 199,\n",
       " 11016: 199,\n",
       " 11017: 199,\n",
       " 11018: 199,\n",
       " 11019: 199,\n",
       " 11020: 199,\n",
       " 11021: 199,\n",
       " 11022: 199,\n",
       " 11023: 199,\n",
       " 11024: 199,\n",
       " 11025: 199,\n",
       " 11026: 199,\n",
       " 11027: 199,\n",
       " 11028: 199,\n",
       " 11029: 199,\n",
       " 11030: 199,\n",
       " 11031: 199,\n",
       " 11032: 199,\n",
       " 11033: 199,\n",
       " 11034: 199,\n",
       " 11035: 199,\n",
       " 11036: 199,\n",
       " 11037: 199,\n",
       " 11038: 199,\n",
       " 11071: 199,\n",
       " 11073: 199,\n",
       " 11134: 199,\n",
       " 11119: 199,\n",
       " 11106: 199,\n",
       " 11107: 199,\n",
       " 11108: 199,\n",
       " 11109: 199,\n",
       " 11110: 199,\n",
       " 11111: 199,\n",
       " 11112: 199,\n",
       " 11113: 199,\n",
       " 11114: 199,\n",
       " 11115: 199,\n",
       " 11116: 199,\n",
       " 11117: 199,\n",
       " 11118: 199,\n",
       " 11120: 199,\n",
       " 11074: 199,\n",
       " 11121: 199,\n",
       " 11122: 199,\n",
       " 11123: 199,\n",
       " 11124: 199,\n",
       " 11125: 199,\n",
       " 11126: 199,\n",
       " 11127: 199,\n",
       " 11128: 199,\n",
       " 11129: 199,\n",
       " 11130: 199,\n",
       " 11131: 199,\n",
       " 11132: 199,\n",
       " 11133: 199,\n",
       " 11105: 199,\n",
       " 11104: 199,\n",
       " 11103: 199,\n",
       " 11102: 199,\n",
       " 11075: 199,\n",
       " 11076: 199,\n",
       " 11077: 199,\n",
       " 11078: 199,\n",
       " 11079: 199,\n",
       " 11080: 199,\n",
       " 11081: 199,\n",
       " 11082: 199,\n",
       " 11083: 199,\n",
       " 11084: 199,\n",
       " 11085: 199,\n",
       " 11086: 199,\n",
       " 11087: 199,\n",
       " 11088: 199,\n",
       " 11089: 199,\n",
       " 11090: 199,\n",
       " 11091: 199,\n",
       " 11092: 199,\n",
       " 11093: 199,\n",
       " 11094: 199,\n",
       " 11095: 199,\n",
       " 11096: 199,\n",
       " 11097: 199,\n",
       " 11098: 199,\n",
       " 11099: 199,\n",
       " 11100: 199,\n",
       " 11101: 199,\n",
       " 10757: 54,\n",
       " 10753: 48,\n",
       " 10755: 53,\n",
       " 10419: 195,\n",
       " 10406: 191,\n",
       " 10407: 193,\n",
       " 10408: 193,\n",
       " 10409: 193,\n",
       " 10410: 191,\n",
       " 10411: 191,\n",
       " 10412: 193,\n",
       " 10413: 196,\n",
       " 10414: 192,\n",
       " 10415: 193,\n",
       " 10416: 194,\n",
       " 10417: 194,\n",
       " 10418: 194,\n",
       " 10420: 195,\n",
       " 10404: 189,\n",
       " 10421: 195,\n",
       " 10422: 195,\n",
       " 10423: 194,\n",
       " 10424: 194,\n",
       " 10425: 195,\n",
       " 10426: 196,\n",
       " 10427: 195,\n",
       " 10428: 195,\n",
       " 10429: 195,\n",
       " 10430: 195,\n",
       " 10431: 195,\n",
       " 10432: 194,\n",
       " 10433: 194,\n",
       " 10405: 189,\n",
       " 10403: 189,\n",
       " 10435: 194,\n",
       " 10386: 104,\n",
       " 10373: 107,\n",
       " 10374: 106,\n",
       " 10375: 107,\n",
       " 10376: 101,\n",
       " 10377: 100,\n",
       " 10378: 103,\n",
       " 10379: 100,\n",
       " 10380: 101,\n",
       " 10381: 97,\n",
       " 10382: 95,\n",
       " 10383: 98,\n",
       " 10384: 100,\n",
       " 10385: 102,\n",
       " 10387: 106,\n",
       " 10402: 191,\n",
       " 10388: 107,\n",
       " 10389: 113,\n",
       " 10390: 122,\n",
       " 10391: 127,\n",
       " 10392: 129,\n",
       " 10393: 159,\n",
       " 10394: 165,\n",
       " 10396: 176,\n",
       " 10397: 176,\n",
       " 10398: 179,\n",
       " 10399: 181,\n",
       " 10400: 185,\n",
       " 10401: 188,\n",
       " 10434: 194,\n",
       " 10436: 196,\n",
       " 10371: 108,\n",
       " 10483: 191,\n",
       " 10470: 192,\n",
       " 10471: 192,\n",
       " 10472: 192,\n",
       " 10473: 192,\n",
       " 10474: 192,\n",
       " 10475: 191,\n",
       " 10476: 191,\n",
       " 10477: 191,\n",
       " 10478: 191,\n",
       " 10479: 191,\n",
       " 10480: 191,\n",
       " 10481: 191,\n",
       " 10482: 191,\n",
       " 10484: 191,\n",
       " 10468: 192,\n",
       " 10485: 191,\n",
       " 10486: 191,\n",
       " 10487: 192,\n",
       " 10488: 192,\n",
       " 10489: 193,\n",
       " 10490: 193,\n",
       " 10491: 194,\n",
       " 10492: 194,\n",
       " 10493: 194,\n",
       " 10494: 194,\n",
       " 10495: 194,\n",
       " 10496: 194,\n",
       " 10497: 194,\n",
       " 10469: 192,\n",
       " 10467: 192,\n",
       " 10437: 196,\n",
       " 10451: 195,\n",
       " 10438: 197,\n",
       " 10439: 196,\n",
       " 10440: 195,\n",
       " 10441: 194,\n",
       " 10442: 194,\n",
       " 10443: 194,\n",
       " 10444: 194,\n",
       " 10445: 194,\n",
       " 10446: 194,\n",
       " 10447: 194,\n",
       " 10448: 194,\n",
       " 10449: 194,\n",
       " 10450: 194,\n",
       " 10452: 195,\n",
       " 10466: 192,\n",
       " 10453: 194,\n",
       " 10454: 194,\n",
       " 10455: 194,\n",
       " 10456: 194,\n",
       " 10457: 194,\n",
       " 10458: 194,\n",
       " 10459: 193,\n",
       " 10460: 192,\n",
       " 10461: 192,\n",
       " 10462: 192,\n",
       " 10463: 192,\n",
       " 10464: 192,\n",
       " 10465: 192,\n",
       " 10372: 106,\n",
       " 10370: 112,\n",
       " 10499: 187,\n",
       " 10290: 160,\n",
       " 10277: 187,\n",
       " 10278: 187,\n",
       " 10279: 188,\n",
       " 10280: 188,\n",
       " 10281: 188,\n",
       " 10282: 191,\n",
       " 10283: 192,\n",
       " 10284: 192,\n",
       " 10285: 191,\n",
       " 10286: 190,\n",
       " 10287: 191,\n",
       " 10288: 162,\n",
       " 10289: 161,\n",
       " 10291: 158,\n",
       " 10275: 181,\n",
       " 10292: 157,\n",
       " 10293: 153,\n",
       " 10294: 157,\n",
       " 10295: 155,\n",
       " 10296: 153,\n",
       " 10297: 181,\n",
       " 10298: 181,\n",
       " 10299: 185,\n",
       " 10300: 190,\n",
       " 10301: 189,\n",
       " 10302: 189,\n",
       " 10303: 191,\n",
       " 10304: 190,\n",
       " 10276: 186,\n",
       " 10274: 180,\n",
       " 10306: 192,\n",
       " 10258: 84,\n",
       " 10245: 38,\n",
       " 10246: 48,\n",
       " 10247: 51,\n",
       " 10248: 52,\n",
       " 10249: 52,\n",
       " 10250: 52,\n",
       " 10251: 78,\n",
       " 10252: 79,\n",
       " 10253: 79,\n",
       " 10254: 79,\n",
       " 10255: 81,\n",
       " 10256: 82,\n",
       " 10257: 82,\n",
       " 10259: 88,\n",
       " 10273: 179,\n",
       " 10260: 97,\n",
       " 10261: 94,\n",
       " 10262: 100,\n",
       " 10263: 102,\n",
       " 10264: 102,\n",
       " 10265: 108,\n",
       " 10266: 117,\n",
       " 10267: 121,\n",
       " 10268: 125,\n",
       " 10269: 158,\n",
       " 10270: 164,\n",
       " 10271: 167,\n",
       " 10272: 174,\n",
       " 10305: 190,\n",
       " 10307: 193,\n",
       " 10369: 110,\n",
       " 10354: 90,\n",
       " 10341: 56,\n",
       " 10342: 55,\n",
       " 10343: 81,\n",
       " 10344: 81,\n",
       " 10345: 81,\n",
       " 10346: 82,\n",
       " 10347: 82,\n",
       " 10348: 81,\n",
       " 10349: 83,\n",
       " 10350: 85,\n",
       " 10351: 84,\n",
       " 10352: 86,\n",
       " 10353: 88,\n",
       " 10355: 91,\n",
       " 10339: 61,\n",
       " 10356: 95,\n",
       " 10357: 98,\n",
       " 10358: 100,\n",
       " 10359: 101,\n",
       " 10360: 102,\n",
       " 10361: 105,\n",
       " 10362: 110,\n",
       " 10363: 111,\n",
       " 10364: 111,\n",
       " 10365: 112,\n",
       " 10366: 113,\n",
       " 10367: 114,\n",
       " 10368: 114,\n",
       " 10340: 56,\n",
       " 10338: 62,\n",
       " 10308: 193,\n",
       " 10322: 162,\n",
       " 10309: 195,\n",
       " 10310: 197,\n",
       " 10311: 197,\n",
       " 10312: 197,\n",
       " 10313: 197,\n",
       " 10314: 197,\n",
       " 10315: 198,\n",
       " 10316: 197,\n",
       " 10317: 171,\n",
       " 10318: 171,\n",
       " 10319: 168,\n",
       " 10320: 169,\n",
       " 10321: 166,\n",
       " 10323: 161,\n",
       " 10337: 71,\n",
       " 10324: 159,\n",
       " 10325: 152,\n",
       " 10326: 147,\n",
       " 10327: 141,\n",
       " 10328: 142,\n",
       " 10329: 132,\n",
       " 10330: 100,\n",
       " 10331: 117,\n",
       " 10332: 88,\n",
       " 10333: 82,\n",
       " 10334: 77,\n",
       " 10335: 73,\n",
       " 10336: 71,\n",
       " 10498: 194,\n",
       " 10500: 180,\n",
       " 10754: 52,\n",
       " 10675: 38,\n",
       " 10662: 51,\n",
       " 10663: 52,\n",
       " 10664: 52,\n",
       " 10665: 52,\n",
       " 10666: 51,\n",
       " 10667: 51,\n",
       " 10668: 51,\n",
       " 10669: 51,\n",
       " 10670: 50,\n",
       " 10671: 47,\n",
       " 10672: 46,\n",
       " 10673: 45,\n",
       " 10674: 43,\n",
       " 10676: 29,\n",
       " 10660: 37,\n",
       " 10677: 26,\n",
       " 10678: 26,\n",
       " 10679: 26,\n",
       " 10680: 26,\n",
       " 10681: 26,\n",
       " 10682: 26,\n",
       " 10683: 26,\n",
       " 10684: 26,\n",
       " 10685: 26,\n",
       " 10686: 26,\n",
       " 22504: 63,\n",
       " 678: 129,\n",
       " 6023: 37,\n",
       " 19964: 26,\n",
       " 20977: 30,\n",
       " 455: 55,\n",
       " 22857: 54,\n",
       " 15943: 53,\n",
       " 29223: 38,\n",
       " 24940: 76,\n",
       " 15763: 26,\n",
       " 14511: 31,\n",
       " 8480: 83,\n",
       " 23065: 26,\n",
       " 9258: 26,\n",
       " 9747: 48,\n",
       " 27755: 44,\n",
       " 19684: 26,\n",
       " 17730: 35,\n",
       " 25959: 32,\n",
       " 20831: 28,\n",
       " 22922: 37,\n",
       " 17072: 28,\n",
       " 8465: 27,\n",
       " 17921: 26,\n",
       " 16722: 26,\n",
       " 83: 26,\n",
       " 7495: 37,\n",
       " 11335: 31,\n",
       " 31903: 26,\n",
       " 23569: 40,\n",
       " 24255: 86,\n",
       " 8096: 33,\n",
       " 20416: 26,\n",
       " 29507: 26,\n",
       " 25917: 26,\n",
       " 22088: 27,\n",
       " 24765: 26,\n",
       " 25386: 28,\n",
       " 15639: 27,\n",
       " 26086: 26,\n",
       " 978: 27,\n",
       " 19317: 26,\n",
       " 25159: 26,\n",
       " 28318: 29,\n",
       " 23535: 27,\n",
       " 5471: 49,\n",
       " 13496: 26,\n",
       " 23096: 95,\n",
       " 28267: 62,\n",
       " 22727: 26,\n",
       " 3049: 68,\n",
       " 17982: 26,\n",
       " 1669: 118,\n",
       " 32514: 34,\n",
       " 10697: 54,\n",
       " 15710: 35,\n",
       " 20706: 63,\n",
       " 24864: 28,\n",
       " 20949: 28,\n",
       " 27373: 28,\n",
       " 23378: 26,\n",
       " 12194: 32,\n",
       " 32374: 42,\n",
       " 28642: 66,\n",
       " 9273: 32,\n",
       " 23710: 26,\n",
       " 23909: 26,\n",
       " 12204: 58,\n",
       " 9843: 29,\n",
       " 23676: 27,\n",
       " 18847: 42,\n",
       " 30519: 27,\n",
       " 8659: 33,\n",
       " 22007: 26,\n",
       " 15770: 30,\n",
       " 24727: 26,\n",
       " 19977: 108,\n",
       " 6065: 26,\n",
       " 18930: 29,\n",
       " 11362: 54,\n",
       " 28137: 26,\n",
       " 16306: 26,\n",
       " 22296: 26,\n",
       " 8604: 26,\n",
       " 10601: 32,\n",
       " 24606: 30,\n",
       " 9384: 26,\n",
       " 1863: 28,\n",
       " 72: 31,\n",
       " 4334: 56,\n",
       " 16339: 27,\n",
       " 4249: 28,\n",
       " 8947: 29,\n",
       " 14480: 26,\n",
       " 31681: 29,\n",
       " 19462: 26,\n",
       " 29473: 27,\n",
       " 13134: 26,\n",
       " 7625: 26,\n",
       " 14889: 26,\n",
       " 30515: 81,\n",
       " 2287: 26,\n",
       " 23862: 26,\n",
       " 6007: 31,\n",
       " 1440: 26,\n",
       " 10733: 30,\n",
       " 10504: 140,\n",
       " 10501: 144,\n",
       " 10502: 142,\n",
       " 5755: 61,\n",
       " 140: 26,\n",
       " 29595: 32,\n",
       " 26711: 26,\n",
       " 2984: 53,\n",
       " 17384: 26,\n",
       " 4597: 42,\n",
       " 23460: 26,\n",
       " 9479: 45,\n",
       " 4206: 26,\n",
       " 2258: 27,\n",
       " 25214: 27,\n",
       " 5413: 34,\n",
       " 410: 46,\n",
       " 14928: 26,\n",
       " 513: 28,\n",
       " 16965: 26,\n",
       " 16459: 40,\n",
       " 2259: 26,\n",
       " 24252: 27,\n",
       " 28924: 26,\n",
       " 31080: 32,\n",
       " 29763: 52,\n",
       " 26833: 30,\n",
       " 27516: 27,\n",
       " 20035: 37,\n",
       " 21439: 28,\n",
       " 18477: 26,\n",
       " 1431: 26,\n",
       " 30583: 26,\n",
       " 7661: 26,\n",
       " 26182: 31,\n",
       " 676: 29,\n",
       " 25166: 26,\n",
       " 15048: 49,\n",
       " 13737: 29,\n",
       " 24932: 37,\n",
       " 21622: 26,\n",
       " 1123: 40,\n",
       " 24203: 30,\n",
       " 14062: 31,\n",
       " 17668: 42,\n",
       " 11392: 56,\n",
       " 26214: 51,\n",
       " 7916: 26,\n",
       " 17864: 65,\n",
       " 27783: 28,\n",
       " 7818: 26,\n",
       " 23241: 26,\n",
       " 30911: 29,\n",
       " 7399: 28,\n",
       " 6254: 50,\n",
       " 29828: 32,\n",
       " 16352: 30,\n",
       " 3858: 89,\n",
       " 30260: 31,\n",
       " 17908: 31,\n",
       " 30973: 32,\n",
       " 17671: 26,\n",
       " 26398: 26,\n",
       " 13107: 37,\n",
       " 27554: 67,\n",
       " 13872: 27,\n",
       " 15938: 27,\n",
       " 7013: 28,\n",
       " 30292: 27,\n",
       " 8929: 27,\n",
       " 16517: 26,\n",
       " 5464: 29,\n",
       " 12265: 70,\n",
       " 13428: 32,\n",
       " 25436: 53,\n",
       " 12274: 58,\n",
       " 14179: 80,\n",
       " 25767: 26,\n",
       " 1792: 29,\n",
       " 31221: 26,\n",
       " 1397: 26,\n",
       " 27921: 27,\n",
       " 6965: 56,\n",
       " 21107: 29,\n",
       " 11301: 26,\n",
       " 23352: 27,\n",
       " 26144: 27,\n",
       " 7798: 27,\n",
       " 31874: 34,\n",
       " 26113: 47,\n",
       " 18976: 26,\n",
       " 3543: 32,\n",
       " 4006: 26,\n",
       " ...}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyze which features activate for specific tokens\n",
    "# top_n = 800  \n",
    "# top_features = []\n",
    "\n",
    "# for sentence_idx, per_token_features in enumerate(sparse_representations):\n",
    "#     sentence_top_features = []\n",
    "    \n",
    "#     for token_idx, features in enumerate(per_token_features):\n",
    "#         # Extract top N active features for this token\n",
    "#         top_indices = np.argsort(features)[-top_n:][::-1]\n",
    "#         sentence_top_features.append(set(top_indices))\n",
    "    \n",
    "#     top_features.append(sentence_top_features)  # Store per-token top feature indices\n",
    "\n",
    "# # Example: Print feature activations for each token in the first sentence\n",
    "# tokenized_sentence = tokenizer(sentences[0])['input_ids']\n",
    "# decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "# print(\"\\nFeature activations for the first sentence:\")\n",
    "# for token, feature_set in zip(decoded_tokens, top_features[0]):\n",
    "#     print(f\"Token: {token}, Top Features: {list(feature_set)[:10]}\")  # Show top 5 features\n",
    "\n",
    "#     # Example: Print feature activations for each token in the first sentence\n",
    "# tokenized_sentence = tokenizer(sentences[1])['input_ids']\n",
    "# decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "# print(\"\\nFeature activations for the first sentence:\")\n",
    "# for token, feature_set in zip(decoded_tokens, top_features[0]):\n",
    "#     print(f\"Token: {token}, Top Features: {list(feature_set)[:10]}\")  # Show top 5 features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the most frequently occurring features\n",
    "top_common_features = sorted(feature_counts, key=feature_counts.get, reverse=True)[:800]\n",
    "\n",
    "# Create a synthetic sparse vector using these common features\n",
    "synthetic_sparse_vector = np.zeros((32768,))  # Assume dictionary size is 32768\n",
    "for idx in top_common_features:\n",
    "    synthetic_sparse_vector[idx] = 1  # Set these features as active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode sparse vector back into model space\n",
    "synthetic_dense_vector = ae.decode(torch.tensor(synthetic_sparse_vector).float()).detach().cpu()\n",
    "synthetic_dense_vector *= 10  # Experiment with scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input: ['Do', 'Ġyou', 'Ġprefer', 'Ġdogs', 'Ġor', 'Ġcats', '?', 'ĠI', 'Ġprefer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntry:\\n    placeholder_index = decoded_tokens.index(\"<XYZ>\")\\nexcept ValueError:\\n    raise ValueError(f\"Could not find placeholder token in: {decoded_tokens}\")\\n'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new special token\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [\"<XYZ>\"]})\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to include new token\n",
    "\n",
    "masked_sentence = \"Do you prefer dogs or cats? I prefer\"\n",
    "input_ids = tokenizer(masked_sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Convert token IDs to tokens\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(f\"Tokenized input: {decoded_tokens}\")  # Debugging\n",
    "\n",
    "# **Find the placeholder index**\n",
    "\"\"\"\n",
    "try:\n",
    "    placeholder_index = decoded_tokens.index(\"<XYZ>\")\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Could not find placeholder token in: {decoded_tokens}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9097, 16581,   281,   247,   253,   952,   731,   368,  4370, 29286]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' dogs',\n",
       " ' cats',\n",
       " ' to',\n",
       " ' a',\n",
       " ' the',\n",
       " ' people',\n",
       " ' them',\n",
       " ' you',\n",
       " ' dog',\n",
       " ' pets']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Convert input_ids to embeddings\n",
    "model_inputs = model.get_input_embeddings()(input_ids)\n",
    "\n",
    "# Inject synthetic feature vector at the placeholder position\n",
    "# model_inputs[:, placeholder_index, :] = synthetic_dense_vector\n",
    "# Even without injection returns a filler word; e.g., \"the\"\n",
    "\n",
    "# Generate text from modified embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs_embeds=model_inputs)\n",
    "    logits = outputs.logits[:, -1, :]  # Get last token logits\n",
    "    logs, tokens = torch.topk(logits, 10,dim=-1)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "# Decode the predicted token\n",
    "predicted_words = tokenizer.batch_decode([token.item() for token in tokens[0]])\n",
    "\n",
    "predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
