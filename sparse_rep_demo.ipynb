{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dictionary import AutoEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pythia model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ae = AutoEncoder.from_pretrained(\n",
    "    \"dictionaries/pythia-70m-deduped/mlp_out_layer3/10_32768/ae.pt\", \n",
    "    map_location=torch.device('cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output: ['The', 'Ġcapital', 'Ġof', 'ĠRussia', 'Ġis', 'ĠMoscow', '.', 'ĠMoscow', 'Ġis', 'Ġin', 'ĠRussia', '.']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sentences.csv\", delimiter=\",\", encoding=\"utf-8\", quotechar='\"')\n",
    "sentences = df['sentence'].tolist()\n",
    "\n",
    "# Testing tokenizer \n",
    "sentence = \"The capital of Russia is Moscow. Moscow is in Russia.\"\n",
    "tokenized_sentence = tokenizer(sentence)['input_ids']\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "\n",
    "print(\"Tokenized output:\", decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_list = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Hook function to capture activations from the 4th MLP layer.\"\"\"\n",
    "    activation_list.append(output)\n",
    "\n",
    "# Hook 4th MLP layer (index 3)\n",
    "layer_to_hook = model.gpt_neox.layers[3].mlp\n",
    "hook = layer_to_hook.register_forward_hook(hook_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 'The capital of Russia is Moscow.'\n",
      "\n",
      "Processing: 'The capital of China is Beijing.'\n",
      "\n",
      "Processing: 'The capital of Greece is Athens.'\n",
      "\n",
      "Processing: 'The capital of Germany is Berlin.'\n",
      "\n",
      "Processing: 'The capital of France is Paris.'\n",
      "\n",
      "Processing: 'The capital of the United Kingdom is London.'\n",
      "\n",
      "Processing: 'The capital of Japan is Tokyo.'\n",
      "\n",
      "Processing: 'The capital of Egypt is Cairo.'\n",
      "\n",
      "Processing: 'The capital of Italy is Rome.'\n",
      "\n",
      "Processing: 'The capital of Spain is Madrid.'\n",
      "\n",
      "Processing: 'The capital of Portugal is Lisbon.'\n",
      "\n",
      "Processing: 'The capital of Canada is Ottawa.'\n",
      "\n",
      "Processing: 'The capital of Australia is Canberra.'\n",
      "\n",
      "Processing: 'The capital of Brazil is Brasília.'\n",
      "\n",
      "Processing: 'The capital of India is New Delhi.'\n",
      "\n",
      "Processing: 'The capital of the United States is Washington, D.C.'\n",
      "\n",
      "Processing: 'The capital of Argentina is Buenos Aires.'\n",
      "\n",
      "Processing: 'The capital of Mexico is Mexico City.'\n",
      "\n",
      "Processing: 'The capital of South Korea is Seoul.'\n",
      "\n",
      "Processing: 'The capital of Indonesia is Jakarta.'\n",
      "\n",
      "Processing: 'The capital of Thailand is Bangkok.'\n",
      "\n",
      "Processing: 'The capital of Norway is Oslo.'\n",
      "\n",
      "Processing: 'The capital of Sweden is Stockholm.'\n",
      "\n",
      "Processing: 'The capital of Finland is Helsinki.'\n",
      "\n",
      "Processing: 'The capital of Poland is Warsaw.'\n",
      "\n",
      "Processing: 'The capital of Austria is Vienna.'\n"
     ]
    }
   ],
   "source": [
    "feature_vals = {}\n",
    "    \n",
    "for sentence in sentences:\n",
    "    print(f\"\\nProcessing: '{sentence}'\")\n",
    "    input_ids_batch = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    model(**input_ids_batch)  # Forward pass to capture activations\n",
    "    tokenized_sentence = tokenizer(sentence)['input_ids']\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence)\n",
    "    feature_vals[sentence] = {}\n",
    "\n",
    "    if activation_list:\n",
    "        activations = activation_list[-1].squeeze(0)  # Shape: (seq_len, hidden_dim)\n",
    "        sparse_repr = ae.encode(activations).detach().cpu().numpy()\n",
    "        for i in range(np.shape(sparse_repr)[0]):\n",
    "            feature_vals[sentence][decoded_tokens[i]] = {}\n",
    "            token_features = sparse_repr[i,:]\n",
    "            sorted_feature_ind = np.argsort(token_features)[:][::-1]\n",
    "            for ind in sorted_feature_ind:\n",
    "                if token_features[ind] != 0.0:\n",
    "                    feature_vals[sentence][decoded_tokens[i]][ind] = token_features[ind]\n",
    "    activation_list.clear()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
