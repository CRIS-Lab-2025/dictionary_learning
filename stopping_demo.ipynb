{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of bones in the human body is significant.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of bones in the human body.\n",
      "The number of bones in the human body is about 1.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be 1.\n",
      "The number of bones in the human body is not the number of bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is estimated to be around 1.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is a measure of the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be between 0.\n",
      "The number of bones in the human body is the same as the number of bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be between 0.\n",
      "The number of bones in the human body is the number of bones in the human body.\n",
      "The number of bones in the human body is the number of the bones of the human body.\n",
      "The number of bones in the human body is estimated to be around 1,000.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is higher than the number of bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is about 6.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is very high.\n",
      "The number of bones in the human body is estimated to be 1.\n",
      "The number of bones in the human body is estimated to be between 1.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be between 1.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is about 1.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be between 1.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is still unknown, but the number of bones in the body is still unknown.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of bones in the human body.\n",
      "The number of bones in the human body is estimated to be 1.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is a measure of the number of bones in the body.\n",
      "The number of bones in the human body is very small, and the number of bones in the body is very small.\n",
      "The number of bones in the human body is estimated to be approximately 2.\n",
      "The number of bones in the human body is the same as the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be at least 10,000.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is a measure of the number of bones in the body.\n",
      "The number of bones in the human body is about 2.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is a significant factor in the development of the human body.\n",
      "The number of bones in the human body is about 1.\n",
      "The number of bones in the human body is a measure of the size of the human body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is estimated to be at least 1.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is very small.\n",
      "The number of bones in the human body is estimated to be 1.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of the bones that are in the body.\n",
      "The number of bones in the human body is the number of the bones in the body and the number of the bones in the body.\n",
      "The number of bones in the human body is estimated to be between 1.\n",
      "The number of bones in the human body is 1.\n",
      "The number of bones in the human body is the number of the bones that are in the body.\n",
      "The number of bones in the human body is estimated to be between 1.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of the bones that are in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is the same as the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be 2.\n",
      "The number of bones in the human body is estimated to be between 1.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be between 1.\n",
      "The number of bones in the human body is estimated to be between 1.\n",
      "The number of bones in the human body is not known.\n",
      "The number of bones in the human body is a very high proportion of the human body.\n",
      "The number of bones in the human body is the number of the bones that are in the body.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is about 3.\n",
      "The number of bones in the human body is the number of the bones in the body.\n",
      "The number of bones in the human body is a major factor in the development of the human body.\n",
      "The number of bones in the human body is not the number of bones in the body.\n",
      "The number of bones in the human body is a measure of the amount of bone that is lost in the body.\n",
      "The number of bones in the human body is the number of bones in the body.\n",
      "The number of bones in the human body is estimated to be around 1.\n",
      "The number of bones in the human body is estimated to be 1.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Pythia model and tokenizer\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"  # Adjust as necessary\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Define a function to generate text until a period is encountered\n",
    "def generate_until_period(input_text, temperature = 0.5, max_length=50):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Generate tokens until a period is encountered\n",
    "    generated_ids = input_ids\n",
    "    while True:\n",
    "        # Get model logits for the input sequence\n",
    "        logits = model(generated_ids).logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Apply softmax to get probabilities for the next token\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample a token from the probability distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1).squeeze().item()\n",
    "        \n",
    "        # Append the generated token to the sequence\n",
    "        generated_ids = torch.cat([generated_ids, torch.tensor([[next_token_id]], device=device)], dim=-1)\n",
    "        \n",
    "        # Decode the generated token\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        \n",
    "        # Check if the token ends with sentence-ending punctuation\n",
    "        if next_token[-1] in ['.', '!', '?']:\n",
    "            break\n",
    "        \n",
    "        # Stop if the sequence length exceeds max_length\n",
    "        if generated_ids.shape[1] > max_length:\n",
    "            break\n",
    "    \n",
    "    # Decode the full generated sequence\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "input_text = \"The number of bones in the human body is\"\n",
    "gen_sens = []\n",
    "\n",
    "for i in range(100):\n",
    "    generated_text = generate_until_period(input_text, temperature=0.25)\n",
    "    print(generated_text)\n",
    "    gen_sens.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Pythia model and tokenizer\n",
    "model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Adjust as necessary\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate_until_period2(input_text, n, max_length=50):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Generate tokens until a period is encountered\n",
    "    generated_ids = input_ids\n",
    "    prob = []\n",
    "    while True:\n",
    "        # Get model logits for the input sequence\n",
    "        logits = model(generated_ids).logits[:, -1, :]\n",
    "        \n",
    "        # Apply softmax to get probabilities for the next token\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the indices of the two highest logits\n",
    "        top2_indices = torch.topk(logits, n, dim=-1).indices.squeeze(dim=0)\n",
    "        \n",
    "        # Extract the probabilities for the top 2 tokens\n",
    "        top2_probs = probs[0, top2_indices]\n",
    "        \n",
    "        # Normalize the probabilities (they should sum to 1)\n",
    "        top2_probs = top2_probs / top2_probs.sum()\n",
    "\n",
    "        # Sample a token from the top 2 probabilities\n",
    "        next_token_id = torch.multinomial(top2_probs, num_samples=1).item()\n",
    "        prob.append(top2_probs[next_token_id].item())\n",
    "        \n",
    "        # Append the generated token to the sequence\n",
    "        generated_ids = torch.cat([generated_ids, torch.tensor([[top2_indices[next_token_id]]], device=device)], dim=-1)\n",
    "        \n",
    "        # Decode the generated token\n",
    "        next_token = tokenizer.decode(top2_indices[next_token_id].item())\n",
    "        \n",
    "        # Check if the token ends with sentence-ending punctuation\n",
    "        if next_token[-1] in ['.', '!', '?']:\n",
    "            break\n",
    "        \n",
    "        # Stop if the sequence length exceeds max_length\n",
    "        if generated_ids.shape[1] > max_length:\n",
    "            break\n",
    "    \n",
    "    # Decode the full generated sequence\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spinal cord is a complex system of nerves that controls all movement of the entire body, but is not involved in sensory perception or motor control.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The spinal cord is\"\n",
    "num_traj = 1000\n",
    "n=5\n",
    "\n",
    "gen_sens = []\n",
    "\n",
    "p_vals = []\n",
    "\n",
    "for i in range(num_traj):\n",
    "    generated_text, p = generate_until_period2(input_text, n)\n",
    "    print(generated_text)\n",
    "    val = np.prod(p)\n",
    "    p_vals.append(val)\n",
    "    gen_sens.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p, base=2):\n",
    "    \"\"\"Compute Shannon entropy of distribution p (list of floats summing to 1).\"\"\"\n",
    "    log_fn = math.log if base == math.e else (lambda x: math.log(x, base))\n",
    "    H = 0.0\n",
    "    for pi in p:\n",
    "        if pi > 0:\n",
    "            H -= pi * log_fn(pi)\n",
    "    return H\n",
    "\n",
    "def mutual_information(samples, keyword1, keyword2):\n",
    "\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "\n",
    "    count_joint1 = 0\n",
    "    count_joint2 = 0\n",
    "    count_joint3 = 0\n",
    "    count_joint4 = 0\n",
    "\n",
    "\n",
    "    for sen in samples:\n",
    "        if keyword1 in sen:\n",
    "            count1 += 1\n",
    "        \n",
    "        if keyword2 in sen:\n",
    "            count2 += 1\n",
    "\n",
    "        if keyword1 in sen and keyword2 in sen:\n",
    "            count_joint1 += 1\n",
    "        \n",
    "        if keyword1 not in sen and keyword2 not in sen:\n",
    "            count_joint2 += 1\n",
    "\n",
    "        if keyword1 in sen and keyword2 not in sen:\n",
    "            count_joint3 += 1\n",
    "        \n",
    "        if keyword1 not in sen and keyword2 in sen:\n",
    "            count_joint4 += 1\n",
    "\n",
    "\n",
    "    prob1 = count1 / num_traj\n",
    "    prob2 = count2 / num_traj\n",
    "    prob1and2 = count_joint1 / num_traj\n",
    "    prob1not2 = count_joint3 / num_traj\n",
    "    prob2not1 = count_joint4 / num_traj\n",
    "    probno1no2 = count_joint2 / num_traj\n",
    "\n",
    "    joint_prob = [prob1and2, prob1not2, prob2not1, probno1no2]\n",
    "\n",
    "\n",
    "    proba = [prob1, 1-prob1]\n",
    "    probb = [prob2, 1-prob2]\n",
    "\n",
    "\n",
    "    joint_entropy = entropy(joint_prob)\n",
    "    h_a = entropy(proba)\n",
    "    h_b = entropy(probb)\n",
    "\n",
    "    mi = h_a + h_b - joint_entropy\n",
    "\n",
    "    return mi, h_a, h_b, joint_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['cortex', 'frontal', 'occipital', 'temporal',\n",
    "         'cerebellum', 'basal ganglia', 'nuclei', 'thinking', \n",
    "         'hippocampus', 'membrane', 'spinal cord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frontal & temporal: 0.8711705334550619\n",
      "cortex & temporal: 0.6484066440162966\n",
      "cortex & frontal: 0.6371206852362756\n",
      "occipital & temporal: 0.5825237870552784\n",
      "frontal & occipital: 0.5482914806687367\n",
      "frontal & cerebellum: 0.5234368691943054\n",
      "temporal & cerebellum: 0.51991980606729\n",
      "temporal & basal ganglia: 0.5182527419439857\n",
      "frontal & basal ganglia: 0.5092904023082714\n",
      "cortex & basal ganglia: 0.5057593087109642\n",
      "cortex & cerebellum: 0.5047964458016523\n",
      "cortex & occipital: 0.4477457147782786\n",
      "occipital & cerebellum: 0.36683246944333314\n",
      "occipital & basal ganglia: 0.35620882143846044\n",
      "cerebellum & basal ganglia: 0.3106398309551628\n",
      "temporal & hippocampus: 0.2053523450931365\n",
      "frontal & hippocampus: 0.20230369636418155\n",
      "cortex & hippocampus: 0.20109580216580314\n",
      "cerebellum & hippocampus: 0.16409866522642047\n",
      "cerebellum & spinal cord: 0.1567251642743639\n",
      "occipital & hippocampus: 0.14681204624140598\n",
      "temporal & spinal cord: 0.13134787408777937\n",
      "frontal & spinal cord: 0.12927540895836098\n",
      "cortex & spinal cord: 0.11136378220497045\n",
      "occipital & spinal cord: 0.09137749511804483\n",
      "hippocampus & spinal cord: 0.04646914657145884\n",
      "basal ganglia & spinal cord: 0.015989310076323093\n",
      "temporal & nuclei: 0.00458366458010917\n",
      "frontal & nuclei: 0.004520038603953669\n",
      "cortex & nuclei: 0.004494783346988163\n",
      "occipital & nuclei: 0.0033344603588048027\n",
      "thinking & hippocampus: 0.003226062493905446\n",
      "basal ganglia & hippocampus: 0.0030025509368207803\n",
      "thinking & spinal cord: 0.002349994500425945\n",
      "basal ganglia & nuclei: 0.002207205387606659\n",
      "cerebellum & thinking: 0.0021915308705973136\n",
      "cortex & membrane: 0.001780399983516201\n",
      "cerebellum & membrane: 0.0013394409753064718\n",
      "basal ganglia & membrane: 0.0011311949253171028\n",
      "cortex & thinking: 0.0009170360450359016\n",
      "temporal & thinking: 0.000837284296535179\n",
      "nuclei & spinal cord: 0.0007195922797350596\n",
      "hippocampus & membrane: 0.000493329085901939\n",
      "basal ganglia & thinking: 0.0004002018631705795\n",
      "membrane & spinal cord: 0.000359412180790053\n",
      "frontal & thinking: 0.0002085878121249607\n",
      "cerebellum & nuclei: 0.00019958047205181906\n",
      "nuclei & hippocampus: 0.00016599517633464345\n",
      "nuclei & thinking: 7.566446660467463e-05\n",
      "thinking & membrane: 3.77940756190881e-05\n",
      "occipital & membrane: 2.240033932077168e-05\n",
      "occipital & thinking: 2.1310248457639958e-05\n",
      "temporal & membrane: 1.2830080932335619e-05\n",
      "nuclei & membrane: 1.1576308604870078e-05\n",
      "frontal & membrane: 1.0236285147069424e-05\n"
     ]
    }
   ],
   "source": [
    "miis = []\n",
    "\n",
    "for w1, w2 in itertools.combinations(terms, 2):\n",
    "\n",
    "    mi, h_a, h_b, h_joint = mutual_information(gen_sens, w1, w2)\n",
    "    miis.append(((w1,w2),mi))\n",
    "\n",
    "sorted_miis = sorted(miis, key=lambda x: x[1], reverse=True)  # reverse=True for descending\n",
    "\n",
    "# Print sorted results\n",
    "for (w1, w2), mi in sorted_miis:\n",
    "    print(f\"{w1} & {w2}: {mi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
